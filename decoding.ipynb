{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_search(model, max_length, bos_token_id, eos_token_id, num_beams):\n",
    "    input_ids = torch.full((batch_size, 1), bos_token_id)\n",
    "    finished  = torch.zeros(batch_size, dtype=torch.bool)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        logits = model(input_ids)[:, -1, :] # (batch_size, seq_len, vocab_size) -> (batch_size, vocab_size)\n",
    "        next_token = torch.argmax(logits, dim=-1)\n",
    "        next_token = torch.where(finished, torch.full_like(next_token, eos_token_id), next_token)\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1) # (batch_size, seq_len)  + (batch_size, 1) -> (batch_size, seq_len+1)\n",
    "\n",
    "        finished |= (next_token == eos_token_id)\n",
    "        if torch.all(finished):\n",
    "            break\n",
    "\n",
    "    return input_ids\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def beam_search(model, batch_size, max_new_tokens, bos_token_id, eos_token_id, beam_size):\n",
    "    \"\"\"\n",
    "    Autoregressive beam search (decoder-only).\n",
    "    Shapes in comments use: B=batch_size, K=beam_size, t=seq_len, V=vocab_size.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- init ----\n",
    "    device = next(model.parameters()).device\n",
    "    V = None  # determined after first forward\n",
    "\n",
    "    # start tokens: (B, 1)\n",
    "    bos = torch.full((batch_size, 1), bos_token_id, dtype=torch.long, device=device)\n",
    "\n",
    "    # t=0 forward to seed beams\n",
    "    out0 = model(bos)\n",
    "    logits0 = out0.logits[:, -1, :] if hasattr(out0, \"logits\") else out0[:, -1, :]  # (B, V)\n",
    "    V = logits0.size(-1)\n",
    "    logp0 = torch.log_softmax(logits0, dim=-1)                                      # (B, V)\n",
    "\n",
    "    # top-K tokens to initialize beams\n",
    "    init_scores, init_tokens = logp0.topk(beam_size, dim=-1)                        # (B, K)\n",
    "    seq      = init_tokens.unsqueeze(-1)                                            # (B, K, 1)\n",
    "    scores   = init_scores.clone()                                                  # (B, K)\n",
    "    finished = (init_tokens == eos_token_id)                                        # (B, K)\n",
    "\n",
    "    # ---- iterate steps 1..max_new_tokens-1 ----\n",
    "    for _ in range(1, max_new_tokens):\n",
    "        # flatten beams → run model once for all beams\n",
    "        # input: (B*K, t)\n",
    "        flat_seq = seq.reshape(batch_size * beam_size, -1)\n",
    "\n",
    "        out = model(flat_seq)\n",
    "        logits = out.logits[:, -1, :] if hasattr(out, \"logits\") else out[:, -1, :]  # (B*K, V)\n",
    "        logp   = torch.log_softmax(logits, dim=-1).view(batch_size, beam_size, -1)  # (B, K, V)\n",
    "\n",
    "        # finished beams: only allow EOS to be extended (keep length; no new content)\n",
    "        if eos_token_id is not None:\n",
    "            mask_finished = finished.unsqueeze(-1)                                   # (B, K, 1)\n",
    "            logp = torch.where(mask_finished, torch.full_like(logp, float(\"-inf\")), logp)  # block all\n",
    "            # pick up log-prob of EOS\n",
    "            eos_slice = logp[..., eos_token_id]\n",
    "            # for those finished beams, EOS is the only choice (others are -inf)\n",
    "            eos_slice = torch.where(finished, torch.zeros_like(eos_slice), eos_slice)\n",
    "            logp[..., eos_token_id] = eos_slice\n",
    "\n",
    "        # expand: scores (B,K,1) + logp (B,K,V) -> candidate scores (B,K,V)\n",
    "        candidate = scores.unsqueeze(-1) + logp                                      # (B, K, V)\n",
    "\n",
    "        # select top-K from K*V candidates per batch\n",
    "        flat_scores, flat_idx = candidate.view(batch_size, -1).topk(beam_size, dim=-1)  # (B, K*V)\n",
    "\n",
    "        # map flat indices back to (parent_beam, next_token)\n",
    "        parent = (flat_idx // V).long()                                              # (B, K)\n",
    "        next_t = (flat_idx %  V).long()                                              # (B, K)\n",
    "\n",
    "        # (B, K) → (B, K, t)\n",
    "        gather_idx  = parent.unsqueeze(-1).expand(batch_size, beam_size, seq.size(-1))\n",
    "        # pick up parent sequences\n",
    "        parent_seqs = seq.gather(dim=1, index=gather_idx)\n",
    "\n",
    "        # append chosen token → new beams\n",
    "        seq    = torch.cat([parent_seqs, next_t.unsqueeze(-1)], dim=-1)              # (B, K, t+1)\n",
    "        scores = flat_scores                                                         # (B, K)\n",
    "\n",
    "        # update finished flags (propagate from parent and new EOS)\n",
    "        finished = finished.gather(1, parent) | (next_t == eos_token_id)             # (B, K)\n",
    "\n",
    "        # early stop if all beams in all batches are finished\n",
    "        if torch.all(finished):\n",
    "            break\n",
    "\n",
    "    # pick best beam per batch\n",
    "    best_scores, best_idx = scores.max(dim=1)                                        # (B,)\n",
    "    take = best_idx.view(batch_size, 1, 1).expand(batch_size, 1, seq.size(-1))       # (B,1,t)\n",
    "    best_seq = seq.gather(dim=1, index=take).squeeze(1)                               # (B, t)\n",
    "\n",
    "    return best_seq, best_scores  # also return (seq, scores) if you want all beams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(probs, beam_size, eps=1e-12):\n",
    "    B, T, V = probs.shape\n",
    "    log_probs = (probs + eps).log()             # to log-probs for numerical stability\n",
    "\n",
    "    # t=0: pick top-K tokens for each batch as the initial beams\n",
    "    init_scores, init_tokens = log_probs[:, 0, :].topk(beam_size, dim=-1)  # both (B, K)\n",
    "    seq    = init_tokens.unsqueeze(-1)          # (B, K, 1) store the first token of each beam\n",
    "    scores = init_scores                        # (B, K)    accumulated log-scores per beam\n",
    "\n",
    "    for t in range(1, T):\n",
    "        expanded = scores.unsqueeze(-1) + log_probs[:, t, :].unsqueeze(1)  # (B, K, 1) + (B, 1, V) -> (B, K, V)\n",
    "\n",
    "        # From K*V candidates, keep the best K per batch\n",
    "        flat_scores, flat_indices = expanded.reshape(B, -1).topk(beam_size, dim=-1)  # (B, K*V) -> (B, K)\n",
    "\n",
    "        parent_beam = (flat_indices // V).long()   # (B, K) which previous beam each new candidate came from\n",
    "        next_token  = (flat_indices %  V).long()   # (B, K) which token was chosen at this step\n",
    "\n",
    "        # Gather parent sequences along the beam dimension (dim=1)\n",
    "        # seq: (B, K, t) -> pick the parent beams specified by parent_beam\n",
    "        gather_idx  = parent_beam.unsqueeze(-1).expand(B, beam_size, seq.size(-1))  # (B, K, t)\n",
    "        parent_seqs = seq.gather(dim=1, index=gather_idx)                            # (B, K, t)\n",
    "\n",
    "        # Append the chosen token for this time step to each selected parent sequence\n",
    "        seq    = torch.cat([parent_seqs, next_token.unsqueeze(-1)], dim=-1)  # (B, K, t+1)\n",
    "        scores = flat_scores                                                 # (B, K) update accumulated scores\n",
    "\n",
    "    return seq, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def temperature_sampling(logits, temperature=1.0):\n",
    "    probs = torch.softmax(logits / max(1e-8, float(temperature)), dim=-1) # (B, V)\n",
    "    idx = torch.multinomial(probs, num_samples=1).squeeze(-1)  # [B]\n",
    "    return idx\n",
    "\n",
    "@torch.no_grad()\n",
    "def top_k_sampling(logits, temperature=1.0, top_k=10):\n",
    "    V = logits.size(-1)\n",
    "    scaled_logits = logits / max(1e-8, float(temperature)) # (B, V)\n",
    "\n",
    "    if (not top_k) or (top_k >= V) or (top_k <= 0):\n",
    "        return torch.multinomial(scaled_logits, num_samples=1).squeeze(-1)\n",
    "\n",
    "    topk_logits, topk_indices = torch.topk(scaled_logits, top_k, dim=-1)\n",
    "    filtered_logits = torch.full_like(scaled_logits, -float('inf'))\n",
    "    # use topk_indices to pick topk_logits for each batch\n",
    "    filtered_logits.scatter_(dim=-1, index=topk_indices, src=topk_logits)\n",
    "    # convert logits to probs again\n",
    "    filtered_probs = torch.softmax(filtered_logits, dim=-1)\n",
    "    idx = torch.multinomial(filtered_probs, num_samples=1).squeeze(-1)  # [B]\n",
    "    return idx\n",
    "\n",
    "@torch.no_grad()\n",
    "def top_p_sampling(logits, temperature=1.0, top_p=0.9, min_tokens_to_keep=1):\n",
    "    scaled_logits = logits / max(1e-8, float(temperature))\n",
    "    probs = torch.softmax(scaled_logits, dim=-1)\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, dim=-1, descending=True) # (B, V)\n",
    "    cum_probs = torch.cumsum(sorted_probs, dim=-1) # (B, V)\n",
    "\n",
    "    # includes the token that makes cumulative cross the threshold\n",
    "    keep_mask = (cum_probs - sorted_probs) < top_p\n",
    "\n",
    "    if min_tokens_to_keep > 0:\n",
    "        keep_mask[..., :m] = True\n",
    "\n",
    "    probs = sorted_probs * keep_mask\n",
    "    normalized_probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    sampled_sorted = torch.multinomial(normalized_probs, num_samples=1).squeeze(-1) # [B]\n",
    "    \n",
    "    # convert sorted indices to original indices\n",
    "    idx = torch.gather(sorted_indices, dim=-1, index=sampled_sorted) # [B]\n",
    "    return idx\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
