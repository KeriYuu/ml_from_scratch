{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_search(model, max_length, bos_token_id, eos_token_id, num_beams):\n",
    "    input_ids = torch.full((batch_size, 1), bos_token_id)\n",
    "    finished  = torch.zeros(batch_size, dtype=torch.bool)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        logits = model(input_ids)[:, -1, :] # (batch_size, seq_len, vocab_size) -> (batch_size, vocab_size)\n",
    "        next_token = torch.argmax(logits, dim=-1)\n",
    "        next_token = torch.where(finished, torch.full_like(next_token, eos_token_id), next_token)\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "        finished |= (next_token == eos_token_id)\n",
    "        if torch.all(finished):\n",
    "            break\n",
    "\n",
    "    return input_ids\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(probs, beam_size, eps=1e-12):\n",
    "    B, T, V = probs.shape\n",
    "    log_probs = (probs + eps).log()             # to log-probs for numerical stability\n",
    "\n",
    "    # t=0: pick top-K tokens for each batch as the initial beams\n",
    "    init_scores, init_tokens = log_probs[:, 0, :].topk(beam_size, dim=-1)  # both (B, K)\n",
    "    seq    = init_tokens.unsqueeze(-1)          # (B, K, 1) store the first token of each beam\n",
    "    scores = init_scores                        # (B, K)    accumulated log-scores per beam\n",
    "\n",
    "    for t in range(1, T):\n",
    "        expanded = scores.unsqueeze(-1) + log_probs[:, t, :].unsqueeze(1)  # (B, K, 1) + (B, 1, V) -> (B, K, V)\n",
    "\n",
    "        # From K*V candidates, keep the best K per batch\n",
    "        flat_scores, flat_indices = expanded.reshape(B, -1).topk(beam_size, dim=-1)  # (B, K*V) -> (B, K)\n",
    "\n",
    "        parent_beam = (flat_indices // V).long()   # (B, K) which previous beam each new candidate came from\n",
    "        next_token  = (flat_indices %  V).long()   # (B, K) which token was chosen at this step\n",
    "\n",
    "        # Gather parent sequences along the beam dimension (dim=1)\n",
    "        # seq: (B, K, t) -> pick the parent beams specified by parent_beam\n",
    "        gather_idx  = parent_beam.unsqueeze(-1).expand(B, beam_size, seq.size(-1))  # (B, K, t)\n",
    "        parent_seqs = seq.gather(dim=1, index=gather_idx)                            # (B, K, t)\n",
    "\n",
    "        # Append the chosen token for this time step to each selected parent sequence\n",
    "        seq    = torch.cat([parent_seqs, next_token.unsqueeze(-1)], dim=-1)  # (B, K, t+1)\n",
    "        scores = flat_scores                                                 # (B, K) update accumulated scores\n",
    "\n",
    "    return seq, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_sampling(logits, temperature=1.0):\n",
    "    probs = torch.softmax(logits / max(1e-8, float(temperature)), dim=-1) # (B, V)\n",
    "    idx = torch.multinomial(probs, num_samples=1).squeeze(-1)  # [B]\n",
    "    return idx\n",
    "\n",
    "@torch.no_grad()\n",
    "def top_k_sampling(logits, temperature=1.0, top_k=10):\n",
    "    V = logits.size(-1)\n",
    "    scaled_logits = logits / max(1e-8, float(temperature)) # (B, V)\n",
    "\n",
    "    if (not top_k) or (top_k >= V) or (top_k <= 0):\n",
    "        return torch.multinomial(scaled_logits, num_samples=1).squeeze(-1)\n",
    "\n",
    "    topk_logits, topk_indices = torch.topk(scaled_logits, top_k, dim=-1)\n",
    "    filtered_logits = torch.full_like(scaled_logits, -float('inf'))\n",
    "    filtered_logits.scatter_(dim=-1, index=topk_indices, src=topk_logits)\n",
    "\n",
    "    probs = torch.softmax(filtered_probs, dim=-1)\n",
    "    idx = torch.multinomial(probs, num_samples=1).squeeze(-1)  # [B]\n",
    "    return idx\n",
    "\n",
    "@torch.no_grad()\n",
    "def top_p_sampling(logits, temperature=1.0, top_p=0.9, min_tokens_to_keep=1):\n",
    "    scaled_logits = logits / max(1e-8, float(temperature))\n",
    "    probs = torch.softmax(scaled_logits, dim=-1)\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, dim=-1, descending=True) # (B, V)\n",
    "    cum_probs = torch.cumsum(sorted_probs, dim=-1) # (B, V)\n",
    "\n",
    "    # includes the token that makes cumulative cross the threshold\n",
    "    keep_mask = (cum_probs - sorted_probs) < top_p\n",
    "    m = max(min_tokens_to_keep, 0)\n",
    "\n",
    "    if m > 0:\n",
    "        keep_mask[..., :m] = True\n",
    "\n",
    "    probs = sorted_probs * keep_mask\n",
    "    normalized_probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    sampled_sorted = torch.multinomial(normalized_probs, num_samples=1).squeeze(-1) # [B]\n",
    "    idx = torch.gather(sorted_indices, dim=-1, index=sampled_sorted) # [B]\n",
    "    return idx\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
