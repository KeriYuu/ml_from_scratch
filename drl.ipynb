{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ppo\n",
    "\n",
    "Models: policy (actor), value head (critic), reward model (frozen), reference policy (frozen)\n",
    "\n",
    "\n",
    "* Use the current policy to **generate** responses (collect data).\n",
    "* **Reward** = reward-model score − β × KL(current policy || reference policy).\n",
    "* Use the **value head** to estimate values and compute **advantages** (GAE recommended).\n",
    "* **Freeze/cache** the batch’s old log-probs / advantages / return targets.\n",
    "* Do **multiple small PPO updates** on this batch:\n",
    "\n",
    "  * Recompute new log-probs → **ratio** $r = \\exp(\\text{logp}_\\text{new} - \\text{logp}_\\text{old})$.\n",
    "  * **Clip** $r$ to $[1-\\varepsilon,\\, 1+\\varepsilon]$ → compute the policy loss.\n",
    "  * Do **value regression** + **entropy bonus**.\n",
    "  * **Backprop** and **update parameters**.\n",
    "  * **Monitor KL**; if it gets too large, **early-stop** the epoch/iteration.\n",
    "\n",
    "\n",
    "\n",
    "  policy loss: $\\arg \\max _\\theta \\mathbb{E}_{s \\sim \\nu^\\beta, a \\sim \\pi_{\\theta_k}(\\cdot \\mid s)}\\left[\\min \\left(\\frac{\\pi_\\theta(a \\mid s)}{\\pi_{\\theta_k}(a \\mid s)} A^{\\pi_{\\theta_k}}(s, a), \\operatorname{clip}\\left(\\frac{\\pi_\\theta(a \\mid s)}{\\pi_{\\theta_k}(a \\mid s)}, 1-\\epsilon, 1+\\epsilon\\right) A^{\\pi_{\\theta_k}}(s, a)\\right)\\right]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_reward(\n",
    "    r: torch.Tensor,\n",
    "    kl: torch.Tensor,\n",
    "    kl_coef: float,\n",
    "    action_mask: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    # total_token_reward = terminal_reward_at_EOS + (-kl_coef * KL_token)\n",
    "\n",
    "    B, T = kl.shape\n",
    "    device, dtype = kl.device, kl.dtype\n",
    "\n",
    "    # (-beta * KL) shaping on every valid token\n",
    "    kl_reward = -kl_coef * kl  # negative penalty\n",
    "\n",
    "    # Find EOS index = rightmost 1 in each row of mask\n",
    "    flipped = torch.flip(action_mask.long(), dims=[1])                  # [B, T]\n",
    "    rightmost_from_right = torch.argmax(flipped, dim=1, keepdim=True)   # [B, 1]\n",
    "    eos_idx = (T - 1) - rightmost_from_right                            # [B, 1], long\n",
    "\n",
    "    # Place terminal reward r at EOS position\n",
    "    last_reward = torch.zeros_like(kl, dtype=dtype)\n",
    "    last_reward.scatter_(dim=1, index=eos_idx, src=r.to(dtype).unsqueeze(1)) # (B, T)\n",
    "\n",
    "    # Combine and zero out padding positions\n",
    "    total = (last_reward + kl_reward) * action_mask.to(dtype) # (B, T)\n",
    "    return total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_td_delta(\n",
    "    rewards: torch.Tensor,      # [B, T] per-token rewards (e.g., EOS reward + -beta*KL)\n",
    "    values: torch.Tensor,       # [B, T] V(s_t) from the value head (current parameters)\n",
    "    gamma: float,\n",
    "    action_mask: torch.Tensor,  # [B, T] 1 for valid tokens (incl. EOS), 0 for padding\n",
    ") -> torch.Tensor:\n",
    "    B, T = rewards.shape\n",
    "    device, dtype = rewards.device, rewards.dtype\n",
    "\n",
    "    # Find EOS indices (rightmost 1 in the mask for each row)\n",
    "    flipped = torch.flip(action_mask.long(), dims=[1])                 # [B, T]\n",
    "    rightmost_from_right = torch.argmax(flipped, dim=1, keepdim=True)  # [B, 1]\n",
    "    eos_idx = (T - 1) - rightmost_from_right                           # [B, 1]\n",
    "\n",
    "    #   |   t     |  0  |  1  |  2  |  3  |\n",
    "    #   | :------ | :-: | :-: | :-: | :-: |\n",
    "    #   |  V_t    | 10  |  9  |  7  |  0  |\n",
    "    #   | V_{t+1} |  9  |  7  |  0  |  0  |\n",
    "    V_tp1 = torch.zeros_like(values)\n",
    "    V_tp1[:, :-1] = values[:, 1:]\n",
    "\n",
    "    # done_t is 1 exactly at EOS, else 0 on valid tokens; padding stays 0 but masked later\n",
    "    done = torch.zeros_like(action_mask, dtype=values.dtype)           # [B, T], float 0/1\n",
    "    done.scatter_(dim=1, index=eos_idx, src=torch.ones(B, 1, device=device, dtype=values.dtype))\n",
    "\n",
    "    # δ_t = r_t + γ * (1-done_t) * V_{t+1} - V_t\n",
    "    td = rewards + gamma * (1.0 - done) * V_tp1 - values               # [B, T]\n",
    "\n",
    "    # Zero out padding positions (mask=0) to keep shapes consistent\n",
    "    td = td * action_mask.to(dtype)\n",
    "    return td\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_advantage(gamma: float, lam: float, td_delta: torch.Tensor) -> torch.Tensor:\n",
    "    # td_delta: δ_t = r_t + γ V_{t+1} - V_t\n",
    "    B, T = td_delta.shape\n",
    "    adv = torch.zeros_like(td_delta) \n",
    "    acc = torch.zeros(B, device=td_delta.device, dtype=td_delta.dtype)\n",
    "    for t in range(T - 1, -1, -1):\n",
    "        acc = td_delta[:, t] + gamma * lam * acc # A_t = δ_t + γλ A_{t+1}\n",
    "        adv[:, t] = acc\n",
    "\n",
    "    return adv  \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def normalize_advantage(adv: torch.Tensor, mask: Optional[torch.Tensor] = None, eps: float = 1e-8) -> torch.Tensor:\n",
    "    if mask is None:\n",
    "        mean = adv.mean()\n",
    "        std = adv.std(unbiased=False)\n",
    "    else:\n",
    "        denom = mask.sum().clamp_min(1.0)\n",
    "        mean = (adv * mask).sum() / denom\n",
    "        var = (mask * (adv - mean) ** 2).sum() / denom\n",
    "        std = var.sqrt()\n",
    "\n",
    "    return (adv - mean) / (std + eps)\n",
    "\n",
    "class PolicyLoss(nn.Module):\n",
    "    # L_t = -min( ratio * A_t, clip(ratio, 1-ε, 1+ε) * A_t )\n",
    "    #   ratio = π_θ(a_t | s_t) / π_{θ_old}(a_t | s_t) = exp(log_prob_new - log_prob_old)\n",
    "    def __init__(self, clip_ratio: float, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.clip_ratio = float(clip_ratio)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        old_log_prob: torch.Tensor,  # [B, T]\n",
    "        log_prob: torch.Tensor,      # [B, T]\n",
    "        advantage: torch.Tensor,     # [B, T] (should be detached/treated as constant)\n",
    "        action_mask: Optional[torch.Tensor] = None,  # [B, T] 0/1\n",
    "    ) -> torch.Tensor:\n",
    "        advantage = advantage.detach()\n",
    "        ratio = torch.exp(log_prob - old_log_prob)                     # [B, T]\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantage\n",
    "        loss_t = -torch.min(surr1, surr2)                              # [B, T]\n",
    "\n",
    "        if action_mask is not None:\n",
    "            denom = action_mask.sum(dim=-1).clamp_min(self.eps)        # [B]\n",
    "            loss = (loss_t * action_mask).sum(dim=-1) / denom          # [B]\n",
    "        else:\n",
    "            loss = loss_t.mean(dim=-1)                                  # [B]\n",
    "\n",
    "        return loss.mean()  # scalar\n",
    "\n",
    "\n",
    "class ValueLoss(nn.Module):\n",
    "    # MSE between values and (A + V_old)\n",
    "    def __init__(self, clip_ratio: Optional[float], eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        values: torch.Tensor,         # [B, T]\n",
    "        old_values: torch.Tensor,     # [B, T]\n",
    "        return_targets: torch.Tensor, # [B, T]  (e.g., A + V or discounted returns)\n",
    "        action_mask: Optional[torch.Tensor] = None,  # [B, T] 0/1\n",
    "    ) -> torch.Tensor:\n",
    "        if self.clip_ratio is not None:\n",
    "            v_clipped = old_values + torch.clamp(values - old_values, -self.clip_ratio, self.clip_ratio)\n",
    "            mse_unclipped = (values - return_targets).pow(2)\n",
    "            mse_clipped   = (v_clipped - return_targets).pow(2)\n",
    "            loss_t = torch.max(mse_unclipped, mse_clipped)\n",
    "        else:\n",
    "            loss_t = (values - return_targets).pow(2)\n",
    "\n",
    "        if action_mask is not None:\n",
    "            denom = action_mask.sum(dim=-1).clamp_min(self.eps)\n",
    "            loss = (loss_t * action_mask).sum(dim=-1) / denom\n",
    "        else:\n",
    "            loss = loss_t.mean(dim=-1)\n",
    "\n",
    "        return 0.5 * loss.mean()  # scalar\n",
    "\n",
    "\n",
    "class PairWiseRMLoss(nn.Module):\n",
    "    # Bradley–Terry style: loss = -log σ( (r_chosen - r_rejected) - margin )\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        chosen_rewards: torch.Tensor,    # [B]\n",
    "        rejected_rewards: torch.Tensor,  # [B]\n",
    "        margin: Optional[float] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        diff = chosen_rewards - rejected_rewards\n",
    "        if margin is not None:\n",
    "            diff = diff - float(margin)\n",
    "        loss = -F.logsigmoid(diff)\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPO\n",
    "\n",
    "$\\mathcal{L}_{\\mathrm{DPO}}\\left(\\pi_\\theta, \\pi_{\\mathrm{ref}}\\right)=-\\mathbb{E}_{\\left(x, y_w, y_l\\right) \\sim \\mathcal{D}}\\left[\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta\\left(y_w \\mid x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_w \\mid x\\right)}-\\beta \\log \\frac{\\pi_\\theta\\left(y_l \\mid x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_l \\mid x\\right)}\\right)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPOLoss(nn.Module):\n",
    "    def __init__(self, beta, label_smoothing=0.0, ipo=False):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.label_smoothing = label_smoothing # soft label\n",
    "        self.ipo = ipo\n",
    "    \n",
    "    def forward(self, policy_chosen_logps, policy_rejected_logps, reference_chosen_logps, reference_rejected_logps):\n",
    "        policy_log_ratio = policy_chosen_logps - policy_rejected_logps  # [B, T]\n",
    "        reference_log_ratio = reference_chosen_logps - reference_rejected_logps\n",
    "        logits = policy_log_ratio - reference_log_ratio\n",
    "        losses = -F.logsigmoid(self.beta * logits) * (1 - self.label_smoothing) -F.logsigmoid(-self.beta * logits) * self.label_smoothing\n",
    "        loss = losses.mean(dim=-1) # mean over sequence\n",
    "        chosen_rewards = self.beta * (policy_chosen_logps - reference_chosen_logps).detach() # no grad\n",
    "        rejected_rewards = self.beta * (policy_rejected_logps - reference_rejected_logps).detach() # no grad\n",
    "        if self.ipo: # Identity Preference Optimization\n",
    "            loss = loss + self.margin * (chosen_rewards - rejected_rewards)\n",
    "        return loss, chosen_rewards, rejected_rewards      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO\n",
    "\n",
    "\n",
    "\n",
    "$\\begin{aligned} & \\mathcal{J}_{G R P O}(\\theta)=\\mathbb{E}\\left[q \\sim P(Q),\\left\\{o_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text {old }}}(O \\mid q)\\right] \\\\ & \\quad \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{\\left|o_i\\right|} \\sum_{t=1}^{\\left|o_i\\right|}\\left\\{\\min \\left[\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(o_{i, t} \\mid q, o_{i,<t}\\right)} \\hat{A}_{i, t}, \\operatorname{clip}\\left(\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i, t}\\right]-\\beta \\mathbb{D}_{K L}\\left[\\pi_\\theta \\| \\pi_{r e f}\\right]\\right\\}\\end{aligned}$\n",
    "\n",
    "$\\mathbb{D}_{K L}\\left[\\pi_\\theta \\| \\pi_{r e f}\\right]=\\frac{\\pi_{r e f}\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}-\\log \\frac{\\pi_{r e f}\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}-1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_grpo_advantages(\n",
    "    rewards: torch.Tensor,   # [B], one scalar reward per completion\n",
    "    group_size: int,\n",
    "    eps: float = 1e-4,\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    B = rewards.shape[0]\n",
    "    if B % group_size != 0:\n",
    "        raise ValueError(\n",
    "            f\"Batch size {B} is not divisible by group_size {group_size}. \"\n",
    "        )\n",
    "\n",
    "    device, dtype = rewards.device, rewards.dtype\n",
    "    num_groups = B // group_size\n",
    "\n",
    "    # Reshape to [num_groups, group_size] so each row is a group of completions\n",
    "    grouped_rewards = rewards.view(num_groups, group_size)  # [num_groups, G]\n",
    "\n",
    "    # Compute mean reward per group: μ\n",
    "    mean = grouped_rewards.mean(dim=1, keepdim=True)        # [num_groups, 1]\n",
    "\n",
    "    advantages_grouped = grouped_rewards - mean             # [num_groups, G]\n",
    "\n",
    "    # Flatten back to [B]\n",
    "    advantages = advantages_grouped.reshape(B)\n",
    "\n",
    "    return advantages\n",
    "\n",
    "\n",
    "\n",
    "class GRPOLoss(nn.Module):\n",
    "    def __init__(self, epsilon: float = 0.2, beta: float = 0.1, reduction: str = \"mean\"):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        per_token_logps: torch.Tensor,\n",
    "        old_per_token_logps: torch.Tensor,\n",
    "        ref_per_token_logps: torch.Tensor,\n",
    "        advantages: torch.Tensor,\n",
    "        mask: torch.Tensor | None = None,\n",
    "    ):\n",
    "\n",
    "        B, T = per_token_logps.shape\n",
    "\n",
    "        # If advantages is [B], broadcast it to [B, T] so every token of a sequence\n",
    "        # shares the same sequence-level advantage.\n",
    "        if advantages.dim() == 1:\n",
    "            advantages = advantages.unsqueeze(1).expand(B, T)\n",
    "        elif advantages.shape != (B, T):\n",
    "            raise ValueError(\"advantages must be shape [B] or [B, T]\")\n",
    "\n",
    "        # If no mask is given, treat all tokens as valid (mask = 1)\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(per_token_logps, dtype=torch.float32)\n",
    "        else:\n",
    "            mask = mask.to(per_token_logps.dtype)\n",
    "\n",
    "\n",
    "        # 1) PPO-style clipped surrogate:\n",
    "        #    ratio = pi_theta(a_t|s_t) / pi_old(a_t|s_t) = exp(logp_new - logp_old)\n",
    "        log_ratio = per_token_logps - old_per_token_logps       \n",
    "        ratio = torch.exp(log_ratio)             \n",
    "\n",
    "        # Clip the ratio to [1 - epsilon, 1 + epsilon]\n",
    "        clipped_ratio = torch.clamp(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon)\n",
    "\n",
    "        # Unclipped and clipped objectives per token: r_t * A_t, clip(r_t) * A_t\n",
    "        # Note: advantage can be positive or negative.\n",
    "        surrogate_unclipped = ratio * advantages\n",
    "        surrogate_clipped = clipped_ratio * advantages\n",
    "\n",
    "        # PPO uses min(r*A, clip(r)*A) to avoid overly large policy updates.\n",
    "        # We *maximize* this surrogate, so in a loss (to minimize) we take the negative.\n",
    "        policy_loss_per_token = -torch.min(surrogate_unclipped, surrogate_clipped)\n",
    "\n",
    "        # 2) KL penalty per token between current policy and reference policy:\n",
    "        #    D_KL[pi_theta || pi_ref] ~= pi_ref/pi_theta - log(pi_ref/pi_theta) - 1\n",
    "        #    Using the log-space form: Δ = log pi_ref - log pi_theta\n",
    "        log_ratio_ref_policy = ref_per_token_logps - per_token_logps  # Δ = log(pi_ref/pi_theta)\n",
    "        ratio_ref_policy = torch.exp(log_ratio_ref_policy)            # pi_ref / pi_theta\n",
    "\n",
    "        # Closed-form per-token KL approximation:\n",
    "        # D_KL = ratio_ref_policy - log_ratio_ref_policy - 1\n",
    "        kl_per_token = ratio_ref_policy - log_ratio_ref_policy - 1.0\n",
    "\n",
    "        # Total per-token loss: PPO surrogate loss + beta * KL penalty\n",
    "        total_per_token_loss = policy_loss_per_token + self.beta * kl_per_token\n",
    "\n",
    "        # 3) Mask out invalid tokens (e.g., padding / prompt) and reduce\n",
    "        total_per_token_loss = total_per_token_loss * mask\n",
    "        kl_per_token = kl_per_token * mask\n",
    "\n",
    "        denom = mask.sum().clamp_min(1.0)\n",
    "        loss = total_per_token_loss.sum() / denom\n",
    "\n",
    "        return loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
