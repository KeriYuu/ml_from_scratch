{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ppo\n",
    "\n",
    "Models: policy (actor), value head (critic), reward model (frozen), reference policy (frozen)\n",
    "\n",
    "\n",
    "* Use the current policy to **generate** responses (collect data).\n",
    "* **Reward** = reward-model score − β × KL(current policy || reference policy).\n",
    "* Use the **value head** to estimate values and compute **advantages** (GAE recommended).\n",
    "* **Freeze/cache** the batch’s old log-probs / advantages / return targets.\n",
    "* Do **multiple small PPO updates** on this batch:\n",
    "\n",
    "  * Recompute new log-probs → **ratio** $r = \\exp(\\text{logp}_\\text{new} - \\text{logp}_\\text{old})$.\n",
    "  * **Clip** $r$ to $[1-\\varepsilon,\\, 1+\\varepsilon]$ → compute the policy loss.\n",
    "  * Do **value regression** + **entropy bonus**.\n",
    "  * **Backprop** and **update parameters**.\n",
    "  * **Monitor KL**; if it gets too large, **early-stop** the epoch/iteration.\n",
    "\n",
    "\n",
    "\n",
    "  policy loss: $\\arg \\max _\\theta \\mathbb{E}_{s \\sim \\nu^\\beta, a \\sim \\pi_{\\theta_k}(\\cdot \\mid s)}\\left[\\min \\left(\\frac{\\pi_\\theta(a \\mid s)}{\\pi_{\\theta_k}(a \\mid s)} A^{\\pi_{\\theta_k}}(s, a), \\operatorname{clip}\\left(\\frac{\\pi_\\theta(a \\mid s)}{\\pi_{\\theta_k}(a \\mid s)}, 1-\\epsilon, 1+\\epsilon\\right) A^{\\pi_{\\theta_k}}(s, a)\\right)\\right]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantage(gamma, lam, td_delta):\n",
    "    # A_t = sum_l ((lambda * gamma)^l * td_delta_{t+l})\n",
    "    td_delta = td_delta.detach().numpy()\n",
    "    advantage = []\n",
    "    acc = 0\n",
    "    for delta in td_delta[::-1]:\n",
    "        acc = delta + gamma * lam * acc\n",
    "        advantage.append(acc)\n",
    "    advantage.reverse()\n",
    "    return torch.tensor(advantage, dtype=torch.float)\n",
    "\n",
    "def compute_reward(r, kl, kl_coef, action_mask):\n",
    "    # token-level reward shaping\n",
    "    kl_reward = - kl_coef * kl \n",
    "    eos_index = action_mask.size(1) - 1 - action_mask.long().fliplr().argmax(dim=1, keepdim=True)\n",
    "    last_reward = torch.zeros_like(kl).scatter_(dim=1, index=eos_indices, src=r.unsqueeze(1).to(kl.dtype))\n",
    "    return last_reward - kl_reward\n",
    "        \n",
    "class PolicyLoss(nn.Module):\n",
    "    def __init__(self, clip_ratio):\n",
    "        super().__init__()\n",
    "        self.clip_ratio = clip_ratio\n",
    "    \n",
    "    def forward(self, old_log_prob, log_prob, advantage, action_mask):\n",
    "        ratio = torch.exp(log_prob - old_log_prob)\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantage\n",
    "        loss = -torch.min(surr1, surr2)\n",
    "        if action_mask is not None:\n",
    "            loss = (loss * action_mask).sum(axis=-1) / action_mask.sum(axis=-1)\n",
    "        else:\n",
    "            loss = loss.mean(axis=-1) # one loss per sample\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class ValueLoss(nn.Module):\n",
    "    def __init__(self, clip_ratio):\n",
    "        super().__init__()\n",
    "        self.clip_ratio = clip_ratio\n",
    "        \n",
    "    def forward(self, values, old_values, return_targets, action_mask): # targets = A_t + V_t\n",
    "        if self.clip_ratio is not None:\n",
    "            v_clipped = old_values + torch.clamp(values - old_values, -self.clip_ratio, self.clip_ratio)\n",
    "            loss = torch.max((values - return_targets).pow(2), (v_clipped - return_targets).pow(2))\n",
    "        else:\n",
    "            loss = (values - return_targets).pow(2)\n",
    "        if action_mask is not None:\n",
    "            loss = (loss * action_mask).sum(axis=-1) / action_mask.sum(axis=-1)\n",
    "        else:\n",
    "            loss = loss.mean(axis=-1) # one loss per sample\n",
    "        return 0.5 * loss.mean()\n",
    "\n",
    "class PairWiseRMLoss(nn.Module):\n",
    "    def forward(self, chosen_rewards, rejected_rewards, margin):\n",
    "        if margin is not None:\n",
    "            loss = -F.logsigmoid(chosen_rewards - rejected_rewards - margin)\n",
    "        else:\n",
    "            loss = -F.logsigmoid(chosen_rewards - rejected_rewards)\n",
    "        return loss.mean() \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPO\n",
    "\n",
    "$\\mathcal{L}_{\\mathrm{DPO}}\\left(\\pi_\\theta, \\pi_{\\mathrm{ref}}\\right)=-\\mathbb{E}_{\\left(x, y_w, y_l\\right) \\sim \\mathcal{D}}\\left[\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta\\left(y_w \\mid x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_w \\mid x\\right)}-\\beta \\log \\frac{\\pi_\\theta\\left(y_l \\mid x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_l \\mid x\\right)}\\right)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPOLoss(nn.Module):\n",
    "    def __init__(self, beta, label_smoothing=0.0, ipo=False):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.label_smoothing = label_smoothing # soft label\n",
    "        self.ipo = ipo\n",
    "    \n",
    "    def forward(self, policy_chosen_logps, policy_rejected_logps, reference_chosen_logps, reference_rejected_logps):\n",
    "        policy_log_ratio = policy_chosen_logps - policy_rejected_logps  \n",
    "        reference_log_ratio = reference_chosen_logps - reference_rejected_logps\n",
    "        logits = policy_log_ratio - reference_log_ratio\n",
    "        losses = -F.logsigmoid(self.beta * logits) * (1 - self.label_smoothing) -F.logsigmoid(-self.beta * logits) * self.label_smoothing\n",
    "        loss = losses.mean(dim=-1)\n",
    "        chosen_rewards = self.beta * (policy_chosen_logps - reference_chosen_logps).detach()\n",
    "        rejected_rewards = self.beta * (policy_rejected_logps - reference_rejected_logps).detach()\n",
    "        if self.ipo:\n",
    "            loss = loss + self.margin * (chosen_rewards - rejected_rewards)\n",
    "        return loss, chosen_rewards, rejected_rewards      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO\n",
    "\n",
    "\n",
    "\n",
    "$\\begin{aligned} & \\mathcal{J}_{G R P O}(\\theta)=\\mathbb{E}\\left[q \\sim P(Q),\\left\\{o_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text {old }}}(O \\mid q)\\right] \\\\ & \\quad \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{\\left|o_i\\right|} \\sum_{t=1}^{\\left|o_i\\right|}\\left\\{\\min \\left[\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(o_{i, t} \\mid q, o_{i,<t}\\right)} \\hat{A}_{i, t}, \\operatorname{clip}\\left(\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i, t}\\right]-\\beta \\mathbb{D}_{K L}\\left[\\pi_\\theta \\| \\pi_{r e f}\\right]\\right\\}\\end{aligned}$\n",
    "\n",
    "$\\mathbb{D}_{K L}\\left[\\pi_\\theta \\| \\pi_{r e f}\\right]=\\frac{\\pi_{r e f}\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}-\\log \\frac{\\pi_{r e f}\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}-1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, inputs):\n",
    "    prompt_ids, prompt_mask = inputs['prompt_ids'], inputs['prompt_mask']\n",
    "    completion_ids, completion_mask = inputs['completion_ids'], inputs['completion_mask']\n",
    "    input_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n",
    "    attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)\n",
    "    logits_to_keep = completion_ids.size(1)\n",
    "\n",
    "    per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask)\n",
    "\n",
    "    ref_per_token_logps = inputs['ref_per_token_logps']\n",
    "\n",
    "    per_token_kl = torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1\n",
    "\n",
    "    advantages = inputs['advantages']\n",
    "\n",
    "    old_per_token_logps = inputs[\"old_per_token_logps\"] if self.num_iterations > 1 else per_token_logps.detach()\n",
    "    coef_1 = torch.exp(per_token_logps - old_per_token_logps)   # r = new/old\n",
    "    coef_2 = torch.clamp(coef_1, 1 - self.epsilon, 1 + self.epsilon)  # r clip to [1-ε, 1+ε]\n",
    "\n",
    "    per_token_loss1 = coef_1 * advantages.unsqueeze(1)\n",
    "    per_token_loss2 = coef_2 * advantages.unsqueeze(1)\n",
    "\n",
    "    per_token_loss = -torch.min(per_token_loss1, per_token_loss2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
