{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (..., D)\n",
    "        rms = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).sqrt()\n",
    "        x_norm = x / rms\n",
    "        return self.weight * x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d4c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU MLP: (x W1) ⊙ SiLU(x W3) @ W2\n",
    "    LLaMA typically uses d_ff ≈ 4/3 * emb_dim (rounded).\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, d_ff):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(emb_dim, d_ff, bias=False)  # gate\n",
    "        self.W3 = nn.Linear(emb_dim, d_ff, bias=False)  # up\n",
    "        self.W2 = nn.Linear(d_ff, emb_dim, bias=False)  # down\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.W2(self.W1(x) * F.silu(self.W3(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e5eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, kv_groups, d_ff, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.attn_norm = RMSNorm(emb_dim)\n",
    "        self.attn = MultiGroupAttention(emb_dim, n_heads, kv_groups, dropout=dropout)\n",
    "        self.ff_norm = RMSNorm(emb_dim)\n",
    "        self.ff = FeedForward(emb_dim, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        # --- Attention sub-layer (pre-norm) ---\n",
    "        x_norm_attn = self.attn_norm(x)                    # normalize inputs for attention\n",
    "        attn_out = self.attn(x_norm_attn, kv=None, mask=mask)  # self-attention with mask\n",
    "        x = x + self.dropout(attn_out)                     # residual connection\n",
    "\n",
    "        # --- Feed-forward sub-layer (pre-norm) ---\n",
    "        x_norm_ff = self.ff_norm(x)                        # normalize inputs for MLP\n",
    "        ff_out = self.ff(x_norm_ff)                        # SwiGLU MLP\n",
    "        x = x + self.dropout(ff_out)                       # residual connection\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8533be42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.emb_dim = config.emb_dim\n",
    "        self.max_seq_len = config.max_seq_len\n",
    "\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.emb_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DecoderBlock(config.emb_dim, config.n_heads, config.d_ff,\n",
    "                         dropout=config.dropout, kv_groups=config.kv_groups)\n",
    "            for _ in range(config.n_layers)\n",
    "        ])\n",
    "        self.final_norm = RMSNorm(config.emb_dim)\n",
    "        self.lm_head = nn.Linear(config.emb_dim, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie weights (common practice in LLaMA)\n",
    "        self.lm_head.weight = self.tok_emb.weight\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            elif isinstance(m, RMSNorm):\n",
    "                nn.init.ones_(m.weight)\n",
    "\n",
    "    def _build_attention_mask(self, input_ids, key_padding_mask=None):\n",
    "        B, S = input_ids.shape\n",
    "        device = input_ids.device\n",
    "        causal = torch.tril(torch.ones(S, S, device=device, dtype=torch.bool))  # (S,S)\n",
    "        if key_padding_mask is None:\n",
    "            return causal  # (S,S)\n",
    "        # key_padding_mask True=keep; shape (B,S)\n",
    "        return (key_padding_mask.to(torch.bool).unsqueeze(1) & causal.unsqueeze(0))  # (B, S_q, S_k)\n",
    "\n",
    "    def forward(self, input_ids, key_padding_mask=None):\n",
    "        B, S = input_ids.shape\n",
    "        if S > self.max_seq_len:\n",
    "            raise ValueError(f\"sequence length {S} exceeds max_seq_len {self.max_seq_len}\")\n",
    "\n",
    "        attn_mask = self._build_attention_mask(input_ids, key_padding_mask)\n",
    "\n",
    "        x = self.tok_emb(input_ids)  # (B,S,D)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, mask=attn_mask)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.lm_head(x)     # (B,S,V)\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
