{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        self.parameter = {'name': 'Dropout', 'p': p}\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.mask = (torch.rand_like(X) > self.p) / (1 - self.p)\n",
    "        return X * self.mask\n",
    "\n",
    "    def backward(self, d_y):\n",
    "        return d_y * self.mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm:\n",
    "    def __init__(self, num_features, eps=1e-5):\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "\n",
    "        self.gamma = torch.ones(1, num_features)\n",
    "        self.beta = torch.zeros(1, num_features)\n",
    "\n",
    "        self.X, self.X_hat, self.rms, self.d_gamma, self.d_beta = None, None, None, None, None\n",
    "\n",
    "        self.parameter = {'name': 'RMSNorm', 'size': [1, num_features]}\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.rms = torch.sqrt(X.pow(2).mean(dim=1, keepdim=True) + self.eps)\n",
    "        self.X_hat = X / self.rms\n",
    "        return self.X_hat * self.gamma + self.beta\n",
    "\n",
    "    def backward(self, d_y):\n",
    "        self.d_gamma = (d_y * self.X_hat).sum(dim=0, keepdim=True)\n",
    "        self.d_beta = d_y.sum(dim=0, keepdim=True)\n",
    "        \n",
    "        d_X_hat = d_y * self.gamma\n",
    "        inv_rms = 1 / self.rms\n",
    "        d_X = inv_rms * d_X_hat - self.X * (self.X * d_X_hat).mean(dim=1, keepdim=True) * inv_rms ** 3\n",
    "        return d_X\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm1d:\n",
    "    def __init__(self, num_features, eps=1e-5):\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "\n",
    "        self.gamma = torch.ones(1, num_features)\n",
    "        self.beta = torch.zeros(1, num_features)\n",
    "\n",
    "        self.X_hat, self.std_inv, self.d_gamma, self.d_beta = None, None, None, None\n",
    "\n",
    "        self.parameter = {'name': 'LayerNorm1d', 'size': [1, num_features]}\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean = X.mean(dim=1, keepdim=True)\n",
    "        var = X.var(dim=1, unbiased=False, keepdim=True)\n",
    "        std = torch.sqrt(var + self.eps)\n",
    "\n",
    "        self.X_hat = (X - mean) / std\n",
    "        self.std_inv = 1 / std\n",
    "\n",
    "        y = self.gamma * self.X_hat + self.beta\n",
    "        return y\n",
    "\n",
    "    def backward(self, d_y):\n",
    "        self.d_gamma = (d_y * self.X_hat).sum(dim=0, keepdim=True)\n",
    "        self.d_beta = d_y.sum(dim=0, keepdim=True)\n",
    "\n",
    "        d_X_hat = self.gamma * d_y\n",
    "        mean_dxhat = d_X_hat.mean(dim=1, keepdim=True)\n",
    "        mean_dxhat_xhat = (d_X_hat * self.X_hat).mean(dim=1, keepdim=True)\n",
    "        d_X = (d_X_hat - mean_dxhat - self.X_hat * mean_dxhat_xhat) * self.std_inv\n",
    "\n",
    "        return d_X\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1d:\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.9):\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.gamma = torch.ones(1, num_features)       # (1, C)\n",
    "        self.beta  = torch.zeros(1, num_features)      # (1, C)\n",
    "        self.running_mean = torch.zeros(1, num_features)\n",
    "        self.running_var  = torch.ones(1, num_features)\n",
    "\n",
    "        # caches\n",
    "        self.X_hat, self.std_inv, self.m, self.d_gamma, self.d_beta = None, None, None, None, None\n",
    "\n",
    "        self.parameter = {'name': 'BatchNorm1d', 'size': [1, num_features]}\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean = X.mean(dim=0, keepdim=True)\n",
    "        var = X.var(dim=0, unbiased=False, keepdim=True)\n",
    "        std = torch.sqrt(var + self.eps)\n",
    "\n",
    "        self.X_hat = (X - mean) / std\n",
    "        self.std_inv = 1 / std\n",
    "        self.m = X.shape[0]\n",
    "\n",
    "        self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
    "        self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
    "\n",
    "        y = self.gamma * self.X_hat + self.beta\n",
    "        return y\n",
    "\n",
    "    def backward(self, d_y): \n",
    "        # y = gamma * X_hat + beta (B, C), X_hat (B, C)\n",
    "        # X_hat = (X - mean) / std\n",
    "        m = self.m\n",
    "        self.d_gamma = (d_y * self.X_hat).sum(dim=0, keepdim=True) # (1, C), Hadamard product, sum each batch's contribution\n",
    "        self.d_beta = d_y.sum(dim=0, keepdim=True) # (1, C)\n",
    "        dX = (1.0 / m) * self.gamma * self.std_inv * (m * d_y - self.d_beta - self.X_hat * self.d_gamma)\n",
    "        return dX\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, input_dim, out_dim, batch_size, activation, lr=1e-3,\n",
    "                 norm=None, bn_momentum=0.9, norm_eps=1e-5, dropout=0.0):\n",
    "        # He-like scaling for stable starts\n",
    "        self.W = torch.randn(input_dim, out_dim) / (input_dim ** 0.5)\n",
    "        self.b = torch.zeros(1, out_dim)\n",
    "        self.activation = activation\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Optional BatchNorm after linear pre-activation\n",
    "        self.bn = BatchNorm1d(out_dim, eps=norm_eps, momentum=bn_momentum) if norm == 'batchnorm' else None\n",
    "        self.ln = LayerNorm1d(out_dim, eps=norm_eps) if norm == 'layernorm' else None\n",
    "        self.rmsn = RMSNorm(out_dim, eps=norm_eps) if norm == 'rmsnorm' else None\n",
    "        self.dropout = Dropout(dropout) if dropout > 0 else None\n",
    "        # Caches\n",
    "        self.X, self.z, self.z_tilde, self.a = None, None, None, None\n",
    "        self.dW, self.db = None, None\n",
    "\n",
    "        self.parameter = {'name': 'Linear',\n",
    "                          'size': [input_dim, out_dim],\n",
    "                          'activation': activation,\n",
    "                          'batchnorm': bool(self.bn is not None),\n",
    "                          'layernorm': bool(self.ln is not None),\n",
    "                          'rmsnorm': bool(self.rmsn is not None),\n",
    "                          'dropout': bool(self.dropout is not None)}\n",
    "\n",
    "    def _activation(self, z):\n",
    "        if self.activation == 'relu':\n",
    "            return torch.maximum(torch.zeros_like(z), z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + torch.exp(-z))\n",
    "        elif self.activation == 'tanh':\n",
    "            return torch.tanh(z)\n",
    "        else:\n",
    "            return z\n",
    "\n",
    "    def _activation_deriv(self, a):\n",
    "        if self.activation == 'relu':\n",
    "            return (a > 0).to(a.dtype)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return a * (1 - a)\n",
    "        elif self.activation == 'tanh':\n",
    "            return 1 - a**2\n",
    "        else:\n",
    "            return torch.ones_like(a)\n",
    "\n",
    "    def set_training(self, mode: bool):\n",
    "        if self.bn is not None:\n",
    "            self.bn.training = mode\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X                                                 # (B, in_dim)\n",
    "        self.z = X @ self.W + self.b                               # pre-activation (B, out_dim)\n",
    "        if self.bn is not None:\n",
    "            self.z_tilde = self.bn.forward(self.z)\n",
    "        elif self.ln is not None:\n",
    "            self.z_tilde = self.ln.forward(self.z)\n",
    "        elif self.rmsn is not None:\n",
    "            self.z_tilde = self.rmsn.forward(self.z)\n",
    "        else:\n",
    "            self.z_tilde = self.z\n",
    "        self.a = self._activation(self.z_tilde)                    # post-activation\n",
    "        out = self.dropout.forward(self.a) if self.dropout else self.a\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_a):\n",
    "        m = self.X.shape[0]                                        # actual batch size (may be < batch_size at last step)\n",
    "        d_a = self.dropout.backward(d_a) if self.dropout else d_a\n",
    "        d_z_tilde = d_a * self._activation_deriv(self.a)           # (B, out_dim)\n",
    "        if self.bn is not None:\n",
    "            d_z = self.bn.backward(d_z_tilde)\n",
    "        elif self.ln is not None:\n",
    "            d_z = self.ln.backward(d_z_tilde)\n",
    "        elif self.rmsn is not None:\n",
    "            d_z = self.rmsn.backward(d_z_tilde)\n",
    "        else:\n",
    "            d_z = d_z_tilde\n",
    "        d_W = self.X.t() @ d_z / m                                 # (in_dim, out_dim)\n",
    "        d_b = d_z.sum(dim=0, keepdim=True) / m                     # (1, out_dim)\n",
    "        d_X = d_z @ self.W.t()                                     # (B, in_dim)\n",
    "\n",
    "        self.dW, self.db = d_W, d_b\n",
    "        return d_X\n",
    "\n",
    "\n",
    "class SoftMax:\n",
    "    def __init__(self):\n",
    "        self.parameter = {'name': 'SoftMax'}\n",
    "        self.y_pred = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        # stable softmax\n",
    "        X_shift = X - X.max(dim=1, keepdim=True).values\n",
    "        exp = torch.exp(X_shift)\n",
    "        partition = exp.sum(dim=1, keepdim=True)\n",
    "        self.y_pred = exp / partition\n",
    "        return self.y_pred\n",
    "\n",
    "    def backward(self, y):\n",
    "        # Assuming y is one-hot (B, C)\n",
    "        return self.y_pred - y\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_dim, output_classes, hidden_sizes, hidden_activations,\n",
    "                 batch_size, lr=1e-3, norm=None, bn_momentum=0.9, norm_eps=1e-5, dropout=0.0):\n",
    "        self.lr = lr\n",
    "        self.hidden_layers = []\n",
    "        for i, hidden_size in enumerate(hidden_sizes):\n",
    "            self.hidden_layers.append(\n",
    "                LinearLayer(input_dim, hidden_size, batch_size, hidden_activations[i], lr=lr,\n",
    "                            norm=norm, bn_momentum=bn_momentum, norm_eps=norm_eps, dropout=dropout)\n",
    "            )\n",
    "            input_dim = hidden_size\n",
    "\n",
    "        # Output layer: no BN, no activation here (Softmax follows)\n",
    "        self.output_layer = LinearLayer(input_dim, output_classes, batch_size, None, lr=lr,\n",
    "                                        norm=None)\n",
    "        self.softmax = SoftMax()\n",
    "\n",
    "    def set_training(self, mode: bool):\n",
    "        for layer in self.hidden_layers:\n",
    "            layer.set_training(mode)\n",
    "        self.output_layer.set_training(mode)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.hidden_layers:\n",
    "            X = layer.forward(X)\n",
    "        X = self.output_layer.forward(X)\n",
    "        X = self.softmax.forward(X)\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        # forward without cache; BN uses running stats (eval)\n",
    "        out = X\n",
    "        for layer in self.hidden_layers:\n",
    "            z = out @ layer.W + layer.b\n",
    "            if layer.bn is not None:\n",
    "                # Use running stats in eval mode\n",
    "                x_hat = (z - layer.bn.running_mean) / torch.sqrt(layer.bn.running_var + layer.bn.eps)\n",
    "                z = layer.bn.gamma * x_hat + layer.bn.beta\n",
    "            elif layer.ln is not None:\n",
    "                ln_mean = z.mean(dim=1, keepdim=True)\n",
    "                ln_var = z.var(dim=1, unbiased=False, keepdim=True)\n",
    "                z = (z - ln_mean) / torch.sqrt(ln_var + layer.ln.eps) * layer.ln.gamma + layer.ln.beta\n",
    "\n",
    "            elif layer.rmsn is not None:\n",
    "                rms = torch.sqrt(z.pow(2).mean(dim=1, keepdim=True) + layer.rmsn.eps)\n",
    "                z = layer.rmsn.gamma * (z / rms) + layer.rmsn.beta\n",
    "\n",
    "            out = layer._activation(z)\n",
    "        z = out @ self.output_layer.W + self.output_layer.b\n",
    "        probs = self.softmax.forward(z)\n",
    "        return probs\n",
    "\n",
    "    def backward(self, y):\n",
    "        d_a = self.softmax.backward(y)\n",
    "        d_a = self.output_layer.backward(d_a)\n",
    "        for layer in reversed(self.hidden_layers):\n",
    "            d_a = layer.backward(d_a)\n",
    "        return d_a\n",
    "\n",
    "    def print_parameters(self):\n",
    "        for i, layer in enumerate(self.hidden_layers + [self.output_layer]):\n",
    "            print(f\"Layer {i}: {layer.parameter}\")\n",
    "            if getattr(layer, 'bn', None) is not None:\n",
    "                print(f\"         BN: {layer.bn.parameter}\")\n",
    "            if getattr(layer, 'ln', None) is not None:\n",
    "                print(f\"         LayerNorm: {layer.ln.parameter}\")\n",
    "            elif getattr(layer, 'rmsn', None) is not None:\n",
    "                print(f\"         RMSNorm: {layer.rmsn.parameter}\")\n",
    "            if getattr(layer, 'dropout', None) is not None:\n",
    "                print(f\"         Dropout: {layer.dropout.parameter}\")\n",
    "\n",
    "    def params_dict(self):\n",
    "        params = {}\n",
    "        for i, layer in enumerate(self.hidden_layers):\n",
    "            params[f\"W_hidden_{i}\"] = layer.W\n",
    "            params[f\"b_hidden_{i}\"] = layer.b\n",
    "            if layer.bn is not None:\n",
    "                params[f\"gamma_{i}\"] = layer.bn.gamma\n",
    "                params[f\"beta_{i}\"]  = layer.bn.beta\n",
    "            if layer.ln is not None:\n",
    "                params[f\"gamma_{i}\"] = layer.ln.gamma\n",
    "                params[f\"beta_{i}\"]  = layer.ln.beta\n",
    "            if layer.rmsn is not None:\n",
    "                params[f\"gamma_{i}\"] = layer.rmsn.gamma\n",
    "                params[f\"beta_{i}\"]  = layer.rmsn.beta\n",
    "        params[\"W_out\"] = self.output_layer.W\n",
    "        params[\"b_out\"] = self.output_layer.b\n",
    "        return params\n",
    "\n",
    "    def grads_dict(self):\n",
    "        grads = {}\n",
    "        for i, layer in enumerate(self.hidden_layers):\n",
    "            grads[f\"W_hidden_{i}\"] = layer.dW\n",
    "            grads[f\"b_hidden_{i}\"] = layer.db\n",
    "            if layer.bn is not None:\n",
    "                grads[f\"gamma_{i}\"] = layer.bn.d_gamma\n",
    "                grads[f\"beta_{i}\"]  = layer.bn.d_beta\n",
    "            if layer.ln is not None:\n",
    "                grads[f\"gamma_{i}\"] = layer.ln.d_gamma\n",
    "                grads[f\"beta_{i}\"]  = layer.ln.d_beta\n",
    "            if layer.rmsn is not None:\n",
    "                grads[f\"gamma_{i}\"] = layer.rmsn.d_gamma\n",
    "                grads[f\"beta_{i}\"]  = layer.rmsn.d_beta\n",
    "        grads[\"W_out\"] = self.output_layer.dW\n",
    "        grads[\"b_out\"] = self.output_layer.db\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        self.params = params  \n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, grads):\n",
    "        # In-place SGD: param <- param - lr * grad\n",
    "        for k in self.params.keys():\n",
    "            g = grads.get(k, None)\n",
    "            if g is None:\n",
    "                continue\n",
    "            self.params[k] -= self.lr * g\n",
    "\n",
    "class AdamW:\n",
    "    def __init__(self, params, lr=1e-3, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        # First/second moments (same device/dtype as each param)\n",
    "        self.m = {k: torch.zeros_like(v) for k, v in params.items()}\n",
    "        self.v = {k: torch.zeros_like(v) for k, v in params.items()}\n",
    "        self.t = 0  # time step\n",
    "\n",
    "    def step(self, grads):\n",
    "        self.t += 1\n",
    "        b1, b2 = self.beta1, self.beta2\n",
    "        # Bias correction terms (scalars)\n",
    "        bc1 = 1.0 - (b1 ** self.t)\n",
    "        bc2 = 1.0 - (b2 ** self.t)\n",
    "\n",
    "        for k, p in self.params.items():\n",
    "            g = grads.get(k, None)\n",
    "            if g is None:\n",
    "                continue\n",
    "\n",
    "            self.m[k] = b1 * self.m[k] + (1.0 - b1) * g\n",
    "            self.v[k] = b2 * self.v[k] + (1.0 - b2) * (g * g)\n",
    "\n",
    "            # Bias correction\n",
    "            m_hat = self.m[k] / bc1\n",
    "            v_hat = self.v[k] / bc2\n",
    "\n",
    "            # Decoupled weight decay\n",
    "            if self.weight_decay != 0.0:\n",
    "                p.add_(p, alpha=-self.lr * self.weight_decay)  # p <- p - lr*wd*p\n",
    "\n",
    "            # Parameter update\n",
    "            denom = torch.sqrt(v_hat) + self.eps\n",
    "            p.addcdiv_(m_hat, denom, value=-self.lr)  # p <- p - lr * m_hat / denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: {'name': 'Linear', 'size': [4, 32], 'activation': 'relu', 'batchnorm': False, 'layernorm': False, 'rmsnorm': True, 'dropout': True}\n",
      "         RMSNorm: {'name': 'RMSNorm', 'size': [1, 32]}\n",
      "         Dropout: {'name': 'Dropout', 'p': 0.3}\n",
      "Layer 1: {'name': 'Linear', 'size': [32, 16], 'activation': 'relu', 'batchnorm': False, 'layernorm': False, 'rmsnorm': True, 'dropout': True}\n",
      "         RMSNorm: {'name': 'RMSNorm', 'size': [1, 16]}\n",
      "         Dropout: {'name': 'Dropout', 'p': 0.3}\n",
      "Layer 2: {'name': 'Linear', 'size': [16, 3], 'activation': None, 'batchnorm': False, 'layernorm': False, 'rmsnorm': False, 'dropout': False}\n",
      "Epoch 10/30  Train Acc: 0.8333  Test Acc: 0.7667\n",
      "Epoch 20/30  Train Acc: 0.9917  Test Acc: 0.9000\n",
      "Epoch 30/30  Train Acc: 0.9833  Test Acc: 0.9000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def predict_classes(mlp, X):\n",
    "    with torch.no_grad():\n",
    "        return torch.argmax(mlp.predict(X), dim=1)\n",
    "\n",
    "def onehot_to_index(Y):\n",
    "    return torch.argmax(Y, dim=1)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "iris = load_iris()\n",
    "X = torch.tensor(iris.data, dtype=torch.float32)\n",
    "y = torch.tensor(iris.target, dtype=torch.long)\n",
    "\n",
    "N = X.shape[0]\n",
    "perm = torch.randperm(N)\n",
    "split = int(0.8 * N)\n",
    "idx_train, idx_test = perm[:split], perm[split:]\n",
    "\n",
    "X_train, X_test = X[idx_train], X[idx_test]\n",
    "y_train_idx, y_test_idx = y[idx_train], y[idx_test]\n",
    "\n",
    "C = int(y.max().item() + 1)\n",
    "y_train = F.one_hot(y_train_idx, num_classes=C).float()\n",
    "y_test = F.one_hot(y_test_idx, num_classes=C).float()\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_classes = C\n",
    "hidden_sizes = [32, 16]\n",
    "hidden_activations = ['relu', 'relu']\n",
    "batch_size = 32\n",
    "lr = 0.01\n",
    "epochs = 30\n",
    "dropout = 0.3\n",
    "norm = 'rmsnorm'\n",
    "\n",
    "mlp = MLP(input_dim, output_classes, hidden_sizes, hidden_activations, batch_size, lr, norm=norm, dropout=dropout)\n",
    "mlp.print_parameters()\n",
    "opt = AdamW(mlp.params_dict(), lr=lr, weight_decay=0.01)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        mlp.forward(X_batch)\n",
    "        mlp.backward(y_batch)\n",
    "        opt.step(mlp.grads_dict())\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        y_train_pred = predict_classes(mlp, X_train)\n",
    "        y_test_pred = predict_classes(mlp, X_test)\n",
    "        train_acc = (y_train_pred == y_train_idx).float().mean().item()\n",
    "        test_acc = (y_test_pred == y_test_idx).float().mean().item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}  Train Acc: {train_acc:.4f}  Test Acc: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
