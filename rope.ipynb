{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple, Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interleaved:\n",
    " - q'(2i-1) = q(2i-1) * cos(phi_i) − q_(2i) * sin(phi_i)\n",
    " - q'(2i) = q(2i-1) * sin(phi_i) + q_(2i) * cos(phi_i)\n",
    "\n",
    "\n",
    "half:\n",
    " - q'i = q_i * cos(phi_i) − q(i+h) * sin(phi_i)\n",
    " - q'(i+h) = q_i * sin(phi_i) + q(i+h) * cos(phi_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rope_cache(seq_len, dim, base=10000.0, layout=\"interleaved\", device=None, dtype=torch.float32):\n",
    "    assert dim % 2 == 0, \"dimension must be even\"\n",
    "    # Frequencies per pair: inv_freq[i] = base^(-(2i)/D)\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, device=device).float() / dim))  # (D/2,)\n",
    "    pos = torch.arange(seq_len, device=device, dtype=torch.float32)                    # (S,)\n",
    "    angles = torch.outer(pos, inv_freq) # angles[m, i] = pos[m] * inv_freq[i] (S, D/2)\n",
    "\n",
    "\n",
    "    # B: batch size, S: sequence length, H: head, D: dimension\n",
    "    if layout == \"interleaved\":\n",
    "        cos = angles.cos()[None, :, None, :].to(dtype)  # (1, S, 1, D/2)\n",
    "        sin = angles.sin()[None, :, None, :].to(dtype)  # (1, S, 1, D/2)\n",
    "    elif layout == \"half\":\n",
    "        angles2 = torch.cat([angles, angles], dim=-1)   # (S, D)\n",
    "        cos = angles2.cos()[None, :, None, :].to(dtype) # (1, S, 1, D)\n",
    "        sin = angles2.sin()[None, :, None, :].to(dtype) # (1, S, 1, D)\n",
    "    else:\n",
    "        raise ValueError(\"layout must be 'interleaved' or 'half'.\")\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "def apply_rope(x, cos_cached, sin_cached, layout=\"interleaved\"):\n",
    "    cos = cos_cached.to(device=x.device, dtype=x.dtype)\n",
    "    sin = sin_cached.to(device=x.device, dtype=x.dtype)\n",
    "\n",
    "    if layout == \"interleaved\":\n",
    "        # Split into even/odd lanes, rotate, then interleave back\n",
    "        x_even, x_odd = x[..., ::2], x[..., 1::2]       # (B,S,H,D/2) each\n",
    "        x_even_r = x_even * cos - x_odd * sin\n",
    "        x_odd_r  = x_even * sin + x_odd * cos\n",
    "        out = torch.empty_like(x)\n",
    "        out[..., ::2] = x_even_r\n",
    "        out[..., 1::2] = x_odd_r\n",
    "        return out\n",
    "    elif layout == \"half\":\n",
    "        # Rotate via [-back | front] trick\n",
    "        h = x.shape[-1] // 2\n",
    "        return x * cos + torch.cat([-x[..., h:], x[..., :h]], dim=-1) * sin\n",
    "    else:\n",
    "        raise ValueError(\"layout must be 'interleaved' or 'half'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max abs diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# ----- fixed permutation to align \"half\" with \"interleaved\" -----\n",
    "def to_even_odd_first(x):\n",
    "    D = x.shape[-1]\n",
    "    idx = torch.cat([torch.arange(0, D, 2), torch.arange(1, D, 2)], 0).to(x.device)\n",
    "    return x[..., idx]\n",
    "\n",
    "def from_even_odd_first(x):\n",
    "    D = x.shape[-1]\n",
    "    idx = torch.cat([torch.arange(0, D, 2), torch.arange(1, D, 2)], 0).to(x.device)\n",
    "    inv = torch.empty_like(idx); inv[idx] = torch.arange(D, device=x.device)\n",
    "    return x[..., inv]\n",
    "\n",
    "# ====== tiny test ======\n",
    "torch.manual_seed(0)\n",
    "B, S, H, D = 2, 16, 8, 64\n",
    "q = torch.randn(B, S, H, D)\n",
    "k = torch.randn(B, S, H, D)\n",
    "\n",
    "# 1) interleaved path\n",
    "cos_i, sin_i = build_rope_cache(S, D, layout=\"interleaved\", dtype=q.dtype, device=q.device)\n",
    "q_i = apply_rope(q, cos_i, sin_i, \"interleaved\")\n",
    "k_i = apply_rope(k, cos_i, sin_i, \"interleaved\")\n",
    "scores_i = torch.einsum(\"b s h d, b t h d -> b h s t\", q_i, k_i)\n",
    "\n",
    "# 2) half path + permutation alignment\n",
    "qP, kP = to_even_odd_first(q), to_even_odd_first(k)\n",
    "cos_h, sin_h = build_rope_cache(S, D, layout=\"half\", dtype=q.dtype, device=q.device)\n",
    "# q_h = apply_rope(q, cos_h, sin_h, \"half\")\n",
    "# k_h = apply_rope(k, cos_h, sin_h, \"half\")\n",
    "q_h = from_even_odd_first(apply_rope(qP, cos_h, sin_h, \"half\"))\n",
    "k_h = from_even_odd_first(apply_rope(kP, cos_h, sin_h, \"half\"))\n",
    "scores_h = torch.einsum(\"b s h d, b t h d -> b h s t\", q_h, k_h)\n",
    "\n",
    "# compare\n",
    "torch.testing.assert_close(scores_i, scores_h, atol=1e-6, rtol=1e-6)\n",
    "print(\"Max abs diff:\", (scores_i - scores_h).abs().max().item())\n",
    "# The dot product is invariant under both rotation and identical permutations, so the \"interleaved\" and \"half\" RoPE implementations produce equivalent attention scores for the same batch of q and k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
