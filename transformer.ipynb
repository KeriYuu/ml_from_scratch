{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, ffn_dim, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(emb_dim, n_heads)\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(emb_dim, ffn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ffn_dim, emb_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        self_attn_output = self.self_attn(x, kv=None, mask=mask)\n",
    "        x = x + self.dropout(self_attn_output) # residual connection\n",
    "        x = self.ln1(x)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        x = self.ln2(x)\n",
    "        return x    \n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, ffn_dim, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        self.ln3 = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.self_attn  = MultiHeadAttention(emb_dim, n_heads)  # decoder self-attn\n",
    "        self.cross_attn = MultiHeadAttention(emb_dim, n_heads)  # encoder-decoder attn\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(emb_dim, ffn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ffn_dim, emb_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, self_mask=None, encoder_decoder_mask=None):\n",
    "        self_attn_output = self.self_attn(x, kv=None, mask=self_mask)\n",
    "        x = x + self.dropout(self_attn_output)\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        encoder_decoder_attn_output = self.cross_attn(q=x, kv=encoder_output, mask=encoder_decoder_mask)\n",
    "        x = x + self.dropout(encoder_decoder_attn_output)\n",
    "        x = self.ln2(x)\n",
    "        \n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        x = self.ln3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_dim, max_len=5000, base=10000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, emb_dim) # (S, D)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (S, 1)\n",
    "        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() * (-math.log(base) / emb_dim)) # (D/2,)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) # (1, S, D)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1] # (B, S, D)\n",
    "        return self.pe[:, :seq_len] # (1, S, D)\n",
    "\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, max_len=5000, base=10000):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0, \"emb_dim must be divisible by n_heads\"\n",
    "        self.base = base\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "        self.max_len = max_len  \n",
    "\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, self.head_dim, 2, dtype=torch.float) / self.head_dim))\n",
    "        pos = torch.arange(max_len, dtype=torch.float).unsqueeze(1)        # (S, 1)\n",
    "        angle = pos * inv_freq                                             # (S, D/2)\n",
    "\n",
    "        cos = angle.cos()[None, :, None, :]  # (1, S, 1, D/2)  \n",
    "        sin = angle.sin()[None, :, None, :]  # (1, S, 1, D/2)\n",
    "\n",
    "        self.register_buffer('cos', cos, persistent=False)\n",
    "        self.register_buffer('sin', sin, persistent=False)\n",
    "\n",
    "    def forward(self, x, start=0):\n",
    "        # x: [B, S, H, head_dim]\n",
    "        B, S, H, D = x.shape\n",
    "        assert D == self.head_dim, \"x.shape[-1] must match head_dim\"\n",
    "        assert start + S <= self.max_len, \"start + S must be less than max_len\"\n",
    "\n",
    "        cos = self.cos[:, start:start+S].to(x.device, dtype=x.dtype)  # (1, S, 1, D/2)\n",
    "        sin = self.sin[:, start:start+S].to(x.device, dtype=x.dtype)  # (1, S, 1, D/2)\n",
    "\n",
    "        x_even, x_odd = x[..., ::2], x[..., 1::2]                     # (B, S, H, D/2)\n",
    "        x_even_r = x_even * cos - x_odd * sin\n",
    "        x_odd_r  = x_even * sin + x_odd * cos\n",
    "\n",
    "        out = torch.empty_like(x)\n",
    "        out[..., ::2] = x_even_r\n",
    "        out[..., 1::2] = x_odd_r\n",
    "        return out\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0, \"emb_dim must be divisible by n_heads\"\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(emb_dim, emb_dim)  # (B, S_q, D) \n",
    "        self.W_K = nn.Linear(emb_dim, emb_dim)  # (B, S_k, D) \n",
    "        self.W_V = nn.Linear(emb_dim, emb_dim)  # (B, S_k, D) \n",
    "\n",
    "        self.W_O = nn.Linear(emb_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.rope = RoPE(emb_dim, n_heads)\n",
    "\n",
    "    def forward(self, x, kv=None, mask=None):\n",
    "        # x:  (B, S_q, D)  queries\n",
    "        # kv: (B, S_k, D)  keys/values source; if None -> self-attention (kv = x)\n",
    "        B, S_q, D = x.shape\n",
    "        if kv is None:\n",
    "            kv = x\n",
    "        S_k = kv.shape[1]\n",
    "\n",
    "        # project Q from x, K/V from kv\n",
    "        q = self.W_Q(x)      # (B, S_q, D)\n",
    "        k = self.W_K(kv)     # (B, S_k, D)\n",
    "        v = self.W_V(kv)     # (B, S_k, D)\n",
    "\n",
    "        # split heads\n",
    "        q = q.view(B, S_q, self.n_heads, self.head_dim)  # (B, S_q, H, h_dim)\n",
    "        k = k.view(B, S_k, self.n_heads, self.head_dim)  # (B, S_k, H, h_dim)\n",
    "        v = v.view(B, S_k, self.n_heads, self.head_dim)  # (B, S_k, H, h_dim)\n",
    "\n",
    "        q = self.rope(q)\n",
    "        k = self.rope(k)\n",
    "\n",
    "        # move heads forward\n",
    "        q = q.transpose(1, 2)  # (B, H, S_q, h_dim)\n",
    "        k = k.transpose(1, 2)  # (B, H, S_k, h_dim)\n",
    "        v = v.transpose(1, 2)  # (B, H, S_k, h_dim)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, H, S_q, S_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            if mask.dtype != torch.bool:\n",
    "                mask = mask != 0\n",
    "\n",
    "            #   keys(j) → 0  1  2  3\n",
    "            # q\\k(i)↓\n",
    "            # 0           1  0  0  0\n",
    "            # 1           1  1  0  0\n",
    "            # 2           1  1  1  0\n",
    "            # 3           1  1  1  1\n",
    "            if mask.ndim == 2 and mask.shape == (S_q, S_k):   # causal/visibility mask\n",
    "                mask = mask.unsqueeze(0).unsqueeze(0)          # (1, 1, S_q, S_k)\n",
    "            elif mask.ndim == 2 and mask.shape == (B, S_k):    # key padding mask over K\n",
    "                mask = mask.unsqueeze(1).unsqueeze(2)          # (B, 1, 1, S_k)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mask shape: {mask.shape}\")\n",
    "            scores = scores.masked_fill(~mask, float(\"-inf\"))  # (B, H, S_q, S_k)\n",
    "\n",
    "        attn_scores = F.softmax(scores, dim=-1)      # for each query over all keys (B, H, S_q, S_k)\n",
    "        attn_scores = self.dropout(attn_scores)\n",
    "\n",
    "        # weighted sum of V\n",
    "        context_vector = torch.matmul(attn_scores, v)               # (B, H, S_q, h_dim)\n",
    "        context_vector = context_vector.transpose(1, 2).contiguous()  # (B, S_q, H, h_dim)\n",
    "        context_vector = context_vector.view(B, S_q, D)             # concat heads -> (B, S_q, D)\n",
    "\n",
    "        out = self.W_O(context_vector)                             # mix head information -> (B, S_q, D)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWithCache(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0, \"emb_dim must be divisible by n_heads\"\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(emb_dim, emb_dim)  \n",
    "        self.W_K = nn.Linear(emb_dim, emb_dim)  \n",
    "        self.W_V = nn.Linear(emb_dim, emb_dim)  \n",
    "\n",
    "        self.W_O = nn.Linear(emb_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rope = RoPE(emb_dim, n_heads)\n",
    "\n",
    "    def forward(self, x, kv=None, past_kv=None, mask=None):\n",
    "        # prefilling: S_q, S_k > 1\n",
    "        # decoding: S_q, S_k = 1, past_kv is not None\n",
    "        B, S_q, D = x.shape\n",
    "        if kv is None:\n",
    "            kv = x\n",
    "        S_k = kv.shape[1]\n",
    "\n",
    "        q = self.W_Q(x)     \n",
    "        k = self.W_K(kv)   \n",
    "        v = self.W_V(kv)     \n",
    "\n",
    "        q = q.view(B, S_q, self.n_heads, self.head_dim)\n",
    "        k = k.view(B, S_k, self.n_heads, self.head_dim)\n",
    "        v = v.view(B, S_k, self.n_heads, self.head_dim)\n",
    "\n",
    "        q = self.rope(q)\n",
    "        k = self.rope(k)\n",
    "\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        if past_kv is not None:\n",
    "            k = torch.cat([past_kv[0], k], dim=2)\n",
    "            v = torch.cat([past_kv[1], v], dim=2)\n",
    "\n",
    "        S_k = k.shape[2]\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim) \n",
    "\n",
    "        if mask is not None:\n",
    "            if mask.dtype != torch.bool:\n",
    "                mask = mask != 0\n",
    "\n",
    "            if mask.ndim == 2 and mask.shape == (S_q, S_k):   # causal/visibility mask\n",
    "                mask = mask.unsqueeze(0).unsqueeze(0)          # (1, 1, S_q, S_k)\n",
    "            elif mask.ndim == 2 and mask.shape == (B, S_k):    # key padding mask over K\n",
    "                mask = mask.unsqueeze(1).unsqueeze(2)          # (B, 1, 1, S_k)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mask shape: {mask.shape}\")\n",
    "            scores = scores.masked_fill(~mask, float(\"-inf\"))  # (B, H, S_q, S_k)\n",
    "\n",
    "        attn_scores = F.softmax(scores, dim=-1)      \n",
    "        attn_scores = self.dropout(attn_scores)\n",
    "\n",
    "        context_vector = torch.matmul(attn_scores, v)          \n",
    "        context_vector = context_vector.transpose(1, 2).contiguous() \n",
    "        context_vector = context_vector.view(B, S_q, D)          \n",
    "\n",
    "        out = self.W_O(context_vector)                             \n",
    "        present_kv = (k, v)\n",
    "        return out, present_kv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0, \"emb_dim must be divisible by n_heads\"\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(emb_dim, emb_dim)  # (B, S_q, D) \n",
    "        self.W_K = nn.Linear(emb_dim, self.head_dim)  #  -> (B, S_k, h_dim) \n",
    "        self.W_V = nn.Linear(emb_dim, self.head_dim)  # -> (B, S_k, h_dim) \n",
    "        self.W_O = nn.Linear(emb_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.rope = RoPE(emb_dim, n_heads)\n",
    "\n",
    "    def forward(self, x, kv=None, mask=None):\n",
    "        B, S_q, D = x.shape\n",
    "        if kv is None:\n",
    "            kv = x\n",
    "        S_k = kv.shape[1]\n",
    "\n",
    "        q = self.W_Q(x)      # (B, S_q, D)\n",
    "        k = self.W_K(kv)     # (B, S_k, h_dim)\n",
    "        v = self.W_V(kv)     # (B, S_k, h_dim)\n",
    "\n",
    "        q = q.view(B, S_q, self.n_heads, self.head_dim)  # (B, S_q, H, h_dim)\n",
    "        k = k.view(B, S_k, 1, self.head_dim)  # (B, S_k, 1, h_dim)\n",
    "        v = v.view(B, S_k, 1, self.head_dim)  # (B, S_k, 1, h_dim)\n",
    "\n",
    "        q = self.rope(q)\n",
    "        k = self.rope(k)\n",
    "\n",
    "        q = q.transpose(1, 2)  # (B, H, S_q, h_dim)\n",
    "        k = k.transpose(1, 2)  # (B, 1, S_k, h_dim)\n",
    "        v = v.transpose(1, 2)  # (B, 1, S_k, h_dim)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, H, S_q, S_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            if mask.dtype != torch.bool:\n",
    "                mask = mask != 0\n",
    "\n",
    "            if mask.ndim == 2 and mask.shape == (S_q, S_k):   # causal/visibility mask\n",
    "                mask = mask.unsqueeze(0).unsqueeze(0)          # (1, 1, S_q, S_k)\n",
    "            elif mask.ndim == 2 and mask.shape == (B, S_k):    # key padding mask over K\n",
    "                mask = mask.unsqueeze(1).unsqueeze(2)          # (B, 1, 1, S_k)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mask shape: {mask.shape}\")\n",
    "            scores = scores.masked_fill(~mask, float(\"-inf\"))  # (B, H, S_q, S_k)\n",
    "\n",
    "        attn_scores = F.softmax(scores, dim=-1)      # for each query over all keys (B, H, S_q, S_k)\n",
    "        attn_scores = self.dropout(attn_scores)\n",
    "\n",
    "        # weighted sum of V\n",
    "        context_vector = torch.matmul(attn_scores, v)               # (B, H, S_q, h_dim)\n",
    "        context_vector = context_vector.transpose(1, 2).contiguous()  # (B, S_q, H, h_dim)\n",
    "        context_vector = context_vector.view(B, S_q, D)             # concat heads -> (B, S_q, D)\n",
    "\n",
    "        out = self.W_O(context_vector)                             # mix head information -> (B, S_q, D)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, kv_groups, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0, \"emb_dim must be divisible by n_heads\"\n",
    "        assert emb_dim % kv_groups == 0, \"emb_dim must be divisible by kv_groups\"\n",
    "        assert n_heads % kv_groups == 0, \"n_heads must be divisible by kv_groups\"\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.kv_groups = kv_groups\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(emb_dim, emb_dim)  # (B, S_q, D) \n",
    "        self.W_K = nn.Linear(emb_dim, self.kv_groups * self.head_dim) \n",
    "        self.W_V = nn.Linear(emb_dim, self.kv_groups * self.head_dim) \n",
    "        self.W_O = nn.Linear(emb_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.rope = RoPE(emb_dim, n_heads)\n",
    "\n",
    "    def forward(self, x, kv=None, mask=None):\n",
    "        B, S_q, D = x.shape\n",
    "        if kv is None:\n",
    "            kv = x\n",
    "        S_k = kv.shape[1]\n",
    "\n",
    "        q = self.W_Q(x)      # (B, S_q, D)\n",
    "        k = self.W_K(kv)     # (B, S_k, h_dim * G)\n",
    "        v = self.W_V(kv)     # (B, S_k, h_dim * G)\n",
    "\n",
    "        q = q.view(B, S_q, self.n_heads, self.head_dim)  # (B, S_q, H, h_dim)\n",
    "        k = k.view(B, S_k, self.kv_groups, self.head_dim)  # (B, S_k, G, h_dim)\n",
    "        v = v.view(B, S_k, self.kv_groups, self.head_dim)  # (B, S_k, G, h_dim)\n",
    "\n",
    "        q = self.rope(q)\n",
    "        k = self.rope(k)\n",
    "\n",
    "        q = q.transpose(1, 2)  # (B, H, S_q, h_dim)\n",
    "        k = k.transpose(1, 2)  # (B, G, S_k, h_dim)\n",
    "        v = v.transpose(1, 2)  # (B, G, S_k, h_dim)\n",
    "\n",
    "        if self.kv_groups != self.n_heads:\n",
    "            k = k.repeat(1, self.n_heads // self.kv_groups, 1, 1) #(B, G, S_k, h_dim) -> (B, H, S_k, h_dim)\n",
    "            v = v.repeat(1, self.n_heads // self.kv_groups, 1, 1)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, H, S_q, S_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            if mask.dtype != torch.bool:\n",
    "                mask = mask != 0\n",
    "\n",
    "            if mask.ndim == 2 and mask.shape == (S_q, S_k):   # causal/visibility mask\n",
    "                mask = mask.unsqueeze(0).unsqueeze(0)          # (1, 1, S_q, S_k)\n",
    "            elif mask.ndim == 2 and mask.shape == (B, S_k):    # key padding mask over K\n",
    "                mask = mask.unsqueeze(1).unsqueeze(2)          # (B, 1, 1, S_k)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mask shape: {mask.shape}\")\n",
    "            scores = scores.masked_fill(~mask, float(\"-inf\"))  # (B, H, S_q, S_k)\n",
    "\n",
    "        attn_scores = F.softmax(scores, dim=-1)      # for each query over all keys (B, H, S_q, S_k)\n",
    "        attn_scores = self.dropout(attn_scores)\n",
    "\n",
    "        # weighted sum of V\n",
    "        context_vector = torch.matmul(attn_scores, v)               # (B, H, S_q, h_dim)\n",
    "        context_vector = context_vector.transpose(1, 2).contiguous()  # (B, S_q, H, h_dim)\n",
    "        context_vector = context_vector.view(B, S_q, D)             # concat heads -> (B, S_q, D)\n",
    "\n",
    "        out = self.W_O(context_vector)                             # mix head information -> (B, S_q, D)\n",
    "        return out\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutiheadLatentAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, down_dim, up_dim, rope_dim, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0, \"emb_dim must be divisible by n_heads\"\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.down_dim = down_dim\n",
    "        self.up_dim = up_dim\n",
    "        self.rope_dim = rope_dim\n",
    "        self.v_head_dim = up_dim // n_heads\n",
    "\n",
    "        self.down_proj_kv = nn.Linear(emb_dim, down_dim)\n",
    "        self.down_proj_q = nn.Linear(emb_dim, down_dim)\n",
    "\n",
    "        self.up_proj_q = nn.Linear(down_dim, up_dim)\n",
    "        self.up_proj_k = nn.Linear(down_dim, up_dim)\n",
    "        self.up_proj_v = nn.Linear(down_dim, up_dim)\n",
    "\n",
    "        self.proj_qr = nn.Linear(down_dim, rope_dim * n_heads)\n",
    "        self.proj_kr_ = nn.Linear(emb_dim, rope_dim)\n",
    "\n",
    "        self.rope_q = RoPE(rope_dim * n_heads, n_heads)\n",
    "        self.rope_k = RoPE(rope_dim, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.res_dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(n_heads * self.v_head_dim, emb_dim)\n",
    "\n",
    "    def forward(self, h, kv=None, mask=None, past_c_kv=None, past_k_t_r=None, start_pos=0):\n",
    "        B, S_q, D = h.shape\n",
    "\n",
    "        if kv is None:\n",
    "            kv = h\n",
    "        S_k = kv.shape[1]\n",
    "\n",
    "        c_t_kv = self.down_proj_kv(kv) # (B, S_k, down_dim)\n",
    "        if past_c_kv is not None:\n",
    "            c_t_kv = torch.cat([past_c_kv, c_t_kv], dim=1)\n",
    "        S_k = c_t_kv.shape[1]\n",
    "\n",
    "        k_t_c = self.up_proj_k(c_t_kv) # (B, S_q, up_dim)\n",
    "        v_t_c = self.up_proj_v(c_t_kv) # (B, S_q, up_dim)\n",
    "\n",
    "        c_t_q = self.down_proj_q(h) # (B, S_q, down_dim)\n",
    "        q_t_c = self.up_proj_q(c_t_q) # (B, S_q, up_dim)\n",
    "\n",
    "        q_t_r = self.proj_qr(c_t_q) # (B, S_q, rope_dim * n_heads)\n",
    "        q_t_r = q_t_r.view(B, S_q, self.n_heads, self.rope_dim).transpose(1, 2) # (B, H, S_q, rope_dim)\n",
    "        q_t_r = self.rope_q(q_t_r, start=start_pos) # (B, H, S_q, rope_dim)\n",
    "\n",
    "        k_t_r = self.proj_kr_(kv).unsqueeze(1) # (B, 1, S_k, rope_dim)\n",
    "        k_t_r = self.rope_k(k_t_r, start=start_pos) # (B, 1, S_k, rope_dim)\n",
    "        if past_k_t_r is not None:\n",
    "            k_t_r = torch.cat([past_k_t_r, k_t_r], dim=2)\n",
    "\n",
    "\n",
    "        q_t_c = q_t_c.view(B, S_q, self.n_heads, self.v_head_dim).transpose(1, 2) # (B, H, S_q, v_head_dim)\n",
    "        q = torch.cat([q_t_c, q_t_r], dim=-1) # (B, H, S_q, rope_dim + v_head_dim)\n",
    "\n",
    "        k_t_c = k_t_c.view(B, S_k, self.n_heads, self.v_head_dim).transpose(1, 2) # (B, H, S_k, v_head_dim)\n",
    "        k = torch.cat([k_t_c, k_t_r.expand(B, self.n_heads, S_k, self.rope_dim)], dim=-1) # (B, H, S_k, rope_dim + v_head_dim)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (math.sqrt(self.rope_dim + self.v_head_dim)) # (B, H, S_q, S_k)\n",
    "        scores = self.dropout(scores)\n",
    "\n",
    "\n",
    "        if mask is not None:\n",
    "            if mask.dtype != torch.bool:\n",
    "                mask = mask != 0\n",
    "\n",
    "            if mask.ndim == 2 and mask.shape == (S_q, S_k):   # causal/visibility mask\n",
    "                mask = mask.unsqueeze(0).unsqueeze(0)          # (1, 1, S_q, S_k)\n",
    "            elif mask.ndim == 2 and mask.shape == (B, S_k):    # key padding mask over K\n",
    "                mask = mask.unsqueeze(1).unsqueeze(2)          # (B, 1, 1, S_k)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mask shape: {mask.shape}\")\n",
    "            scores = scores.masked_fill(~mask, float(\"-inf\"))  # (B, H, S_q, S_k)\n",
    "        \n",
    "        attn_scores = F.softmax(scores, dim=-1) # (B, H, S_q, S_k)\n",
    "        attn_scores = self.dropout(attn_scores)\n",
    "\n",
    "        v_t_c = v_t_c.view(B, S_k, self.n_heads, self.v_head_dim).transpose(1, 2) # (B, H, S_k, v_head_dim)\n",
    "        context_vector = torch.matmul(attn_scores, v_t_c) # (B, H, S_q, v_head_dim)\n",
    "        context_vector = context_vector.transpose(1, 2).contiguous().view(B, S_q, self.up_dim) # (B, S_q, up_dim)\n",
    "\n",
    "        out = self.fc(context_vector) # (B, S_q, D)\n",
    "        out = self.res_dropout(out)\n",
    "        return out, c_t_kv, k_t_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_softmax_update(logits, v_block, m, l, o):\n",
    "    \"\"\"\n",
    "        logits:  (B, H, S_q, block)    attention logits for this block\n",
    "        v_block:(B, H, block, h_dim)  value vectors for this block\n",
    "        m:      (B, H, S_q)           running max over all previous logits\n",
    "        l:      (B, H, S_q)           running sum of exp(logits - m)\n",
    "        o:      (B, H, S_q, h_dim)    running sum of exp(logits - m) * V\n",
    "    \"\"\"\n",
    "    # max logits for this block\n",
    "    m_block = logits.max(dim=-1).values                 # (B, H, S_q)\n",
    "    # new max over previous blocks + this block\n",
    "    m_new = torch.maximum(m, m_block)                   # (B, H, S_q)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    sum_{j in P} exp(s_j - m_new)\n",
    "        = exp(m - m_new) * sum_{j in P} exp(s_j - m)\n",
    "        = exp(m - m_new) * l\n",
    "\n",
    "    sum_{i in B} exp(s_i - m_new)\n",
    "        = sum_i exp_block[i]             # computed directly\n",
    "\n",
    "    => l_new = exp(m - m_new) * l + sum_i exp_block[i]\n",
    "    \"\"\"\n",
    "    # old exp sum, rescaled to the new max\n",
    "    exp_old = torch.exp(m - m_new) * l                  # (B, H, S_q)\n",
    "    # exp values for this block, already shifted by m_new\n",
    "    exp_block = torch.exp(logits - m_new.unsqueeze(-1)) # (B, H, S_q, block)\n",
    "    # new denominator\n",
    "    l_new = exp_old + exp_block.sum(dim=-1)             # (B, H, S_q)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    sum_{j in P} exp(s_j - m_new) v_j\n",
    "        = exp(m - m_new) * o\n",
    "\n",
    "    sum_{i in B} exp(s_i - m_new) v_i\n",
    "        = sum_i exp_block[i] v_i         # matrix product\n",
    "\n",
    "    => o_new = exp(m - m_new) * o + o_block\n",
    "    \"\"\"\n",
    "    # old numerator, rescaled\n",
    "    o_old_scaled = torch.exp(m - m_new).unsqueeze(-1) * o   # (B, H, S_q, h_dim)\n",
    "    # contribution from this block\n",
    "    o_block = torch.matmul(exp_block, v_block)              # (B, H, S_q, h_dim)\n",
    "    # new numerator\n",
    "    o_new = o_old_scaled + o_block                          # (B, H, S_q, h_dim)\n",
    "\n",
    "    return m_new, l_new, o_new\n",
    "\n",
    "\n",
    "class FlashMultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, n_heads, dropout=0.1, block_size=128):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0, \"emb_dim must be divisible by n_heads\"\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.W_Q = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_K = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_V = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_O = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rope = RoPE(emb_dim, n_heads)\n",
    "\n",
    "    def forward(self, x, kv=None, mask=None):\n",
    "        B, S_q, D = x.shape\n",
    "        if kv is None:\n",
    "            kv = x\n",
    "        S_k = kv.shape[1]\n",
    "\n",
    "        q = self.W_Q(x)      # (B, S_q, D)\n",
    "        k = self.W_K(kv)     # (B, S_k, D)\n",
    "        v = self.W_V(kv)     # (B, S_k, D)\n",
    "\n",
    "        q = q.view(B, S_q, self.n_heads, self.head_dim)\n",
    "        k = k.view(B, S_k, self.n_heads, self.head_dim)\n",
    "        v = v.view(B, S_k, self.n_heads, self.head_dim)\n",
    "\n",
    "        q = self.rope(q)\n",
    "        k = self.rope(k)\n",
    "\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        attn_mask = None\n",
    "        if mask is not None:\n",
    "            if mask.dtype != torch.bool:\n",
    "                mask = mask != 0  # non-zero -> True\n",
    "\n",
    "            if mask.ndim == 2 and mask.shape == (S_q, S_k):\n",
    "                attn_mask = mask.unsqueeze(0).unsqueeze(0)     # (1, 1, S_q, S_k)\n",
    "            elif mask.ndim == 2 and mask.shape == (B, S_k):\n",
    "                attn_mask = mask.unsqueeze(1).unsqueeze(2)     # (B, 1, 1, S_k)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mask shape: {mask.shape}\")\n",
    "\n",
    "\n",
    "        scale = 1.0 / math.sqrt(head_dim)\n",
    "\n",
    "        # running max, denominator, numerator\n",
    "        m = q.new_full((B, H, S_q), float(\"-inf\"))        # (B, H, S_q)\n",
    "        l = q.new_zeros((B, H, S_q))                     # (B, H, S_q)\n",
    "        o = q.new_zeros((B, H, S_q, head_dim))           # (B, H, S_q, head_dim)\n",
    "\n",
    "        for start in range(0, S_k, self.block_size):\n",
    "            end = min(start + self.block_size, S_k)\n",
    "\n",
    "            # K/V block: (B, H, block, head_dim)\n",
    "            k_block = k[:, :, start:end, :]\n",
    "            v_block = v[:, :, start:end, :]\n",
    "\n",
    "            # logits for this block: (B, H, S_q, block)\n",
    "            logits = torch.matmul(q, k_block.transpose(-2, -1)) * scale\n",
    "\n",
    "            # apply mask on this key range if needed\n",
    "            if attn_mask is not None:\n",
    "                # (..., S_k) -> (..., block)\n",
    "                mask_block = attn_mask[..., start:end]   # broadcastable\n",
    "                logits = logits.masked_fill(~mask_block, float(\"-inf\"))\n",
    "\n",
    "            # update streaming softmax state\n",
    "            m, l, o = streaming_softmax_update(logits, v_block, m, l, o)\n",
    "\n",
    "        context = o / l.unsqueeze(-1)                     # (B, H, S_q, head_dim)\n",
    "\n",
    "        context = context.transpose(1, 2).contiguous()    # (B, S_q, H, head_dim)\n",
    "        context = context.view(B, S_q, D)                 # (B, S_q, D)\n",
    "\n",
    "        out = self.W_O(context)\n",
    "        out = self.dropout(out)\n",
    "        return out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
