{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, ffn_dim, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(emb_dim, n_heads)\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(emb_dim, ffn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ffn_dim, emb_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        self_attn_output = self.self_attn(x, kv=None, mask=mask)\n",
    "        x = x + self.dropout(self_attn_output) # residual connection\n",
    "        x = self.ln1(x)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        x = self.ln2(x)\n",
    "        return x    \n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, ffn_dim, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        self.ln3 = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.self_attn  = MultiHeadAttention(emb_dim, n_heads)  # decoder self-attn\n",
    "        self.cross_attn = MultiHeadAttention(emb_dim, n_heads)  # encoder-decoder attn\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(emb_dim, ffn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ffn_dim, emb_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, self_mask=None, encoder_decoder_mask=None):\n",
    "        self_attn_output = self.self_attn(x, kv=None, mask=self_mask)\n",
    "        x = x + self.dropout(self_attn_output)\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        encoder_decoder_attn_output = self.cross_attn(q=x, kv=encoder_output, mask=encoder_decoder_mask)\n",
    "        x = x + self.dropout(encoder_decoder_attn_output)\n",
    "        x = self.ln2(x)\n",
    "        \n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        x = self.ln3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_dim, max_len=5000, base=10000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, emb_dim) # (S, D)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (S, 1)\n",
    "        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() * (-math.log(base) / emb_dim)) # (D/2,)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) # (1, S, D)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1] # (B, S, D)\n",
    "        return self.pe[:, :seq_len] # (1, S, D)\n",
    "\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, max_len=5000, base=10000):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0, \"emb_dim must be divisible by n_heads\"\n",
    "        self.base = base\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "        self.max_len = max_len  \n",
    "\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, self.head_dim, 2, dtype=torch.float) / self.head_dim))\n",
    "        pos = torch.arange(max_len, dtype=torch.float).unsqueeze(1)        # (S, 1)\n",
    "        angle = pos * inv_freq                                             # (S, D/2)\n",
    "\n",
    "        cos = angle.cos()[None, :, None, :]  # (1, S, 1, D/2)  \n",
    "        sin = angle.sin()[None, :, None, :]  # (1, S, 1, D/2)\n",
    "\n",
    "        self.register_buffer('cos', cos, persistent=False)\n",
    "        self.register_buffer('sin', sin, persistent=False)\n",
    "\n",
    "    def forward(self, x, start=0):\n",
    "        # x: [B, S, H, head_dim]\n",
    "        B, S, H, D = x.shape\n",
    "        assert D == self.head_dim, \"x.shape[-1] must match head_dim\"\n",
    "        assert start + S <= self.max_len, \"start + S must be less than max_len\"\n",
    "\n",
    "        cos = self.cos[:, start:start+S].to(x.device, dtype=x.dtype)  # (1, S, 1, D/2)\n",
    "        sin = self.sin[:, start:start+S].to(x.device, dtype=x.dtype)  # (1, S, 1, D/2)\n",
    "\n",
    "        x_even, x_odd = x[..., ::2], x[..., 1::2]                     # (B, S, H, D/2)\n",
    "        x_even_r = x_even * cos - x_odd * sin\n",
    "        x_odd_r  = x_even * sin + x_odd * cos\n",
    "\n",
    "        out = torch.empty_like(x)\n",
    "        out[..., ::2] = x_even_r\n",
    "        out[..., 1::2] = x_odd_r\n",
    "        return out\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0, \"emb_dim must be divisible by n_heads\"\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(emb_dim, emb_dim)  # (B, S_q, D) \n",
    "        self.W_K = nn.Linear(emb_dim, emb_dim)  # (B, S_k, D) \n",
    "        self.W_V = nn.Linear(emb_dim, emb_dim)  # (B, S_k, D) \n",
    "\n",
    "        self.W_O = nn.Linear(emb_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, kv=None, mask=None):\n",
    "        # x:  (B, S_q, D)  queries\n",
    "        # kv: (B, S_k, D)  keys/values source; if None -> self-attention (kv = x)\n",
    "        B, S_q, D = x.shape\n",
    "        if kv is None:\n",
    "            kv = x\n",
    "        S_k = kv.shape[1]\n",
    "\n",
    "        # project Q from x, K/V from kv\n",
    "        q = self.W_Q(x)      # (B, S_q, D)\n",
    "        k = self.W_K(kv)     # (B, S_k, D)\n",
    "        v = self.W_V(kv)     # (B, S_k, D)\n",
    "\n",
    "        # split heads\n",
    "        q = q.view(B, S_q, self.n_heads, self.head_dim)  # (B, S_q, H, h_dim)\n",
    "        k = k.view(B, S_k, self.n_heads, self.head_dim)  # (B, S_k, H, h_dim)\n",
    "        v = v.view(B, S_k, self.n_heads, self.head_dim)  # (B, S_k, H, h_dim)\n",
    "\n",
    "        # move heads forward\n",
    "        q = q.transpose(1, 2)  # (B, H, S_q, h_dim)\n",
    "        k = k.transpose(1, 2)  # (B, H, S_k, h_dim)\n",
    "        v = v.transpose(1, 2)  # (B, H, S_k, h_dim)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, H, S_q, S_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            if mask.dtype != torch.bool:\n",
    "                mask = mask != 0\n",
    "\n",
    "            if mask.ndim == 2 and mask.shape == (S_q, S_k):   # causal/visibility mask\n",
    "                mask = mask.unsqueeze(0).unsqueeze(0)          # (1, 1, S_q, S_k)\n",
    "            elif mask.ndim == 2 and mask.shape == (B, S_k):    # key padding mask over K\n",
    "                mask = mask.unsqueeze(1).unsqueeze(2)          # (B, 1, 1, S_k)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mask shape: {mask.shape}\")\n",
    "            scores = scores.masked_fill(~mask, float(\"-inf\"))  # (B, H, S_q, S_k)\n",
    "\n",
    "        attn_scores = F.softmax(scores, dim=-1)      # for each query over all keys (B, H, S_q, S_k)\n",
    "        attn_scores = self.dropout(attn_scores)\n",
    "\n",
    "        # weighted sum of V\n",
    "        context_vector = torch.matmul(attn_scores, v)               # (B, H, S_q, h_dim)\n",
    "        context_vector = context_vector.transpose(1, 2).contiguous()  # (B, S_q, H, h_dim)\n",
    "        context_vector = context_vector.view(B, S_q, D)             # concat heads -> (B, S_q, D)\n",
    "\n",
    "        out = self.W_O(context_vector)                             # mix head information -> (B, S_q, D)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWithCache(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0, \"emb_dim must be divisible by n_heads\"\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(emb_dim, emb_dim)  \n",
    "        self.W_K = nn.Linear(emb_dim, emb_dim)  \n",
    "        self.W_V = nn.Linear(emb_dim, emb_dim)  \n",
    "\n",
    "        self.W_O = nn.Linear(emb_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, kv=None, past_kv=None):\n",
    "        # prefilling: S > 1\n",
    "        # decoding: S = 1 \n",
    "        B, S_q, D = x.shape\n",
    "        if kv is None:\n",
    "            kv = x\n",
    "        S_k = kv.shape[1]\n",
    "\n",
    "        q = self.W_Q(x)     \n",
    "        k = self.W_K(kv)   \n",
    "        v = self.W_V(kv)     \n",
    "\n",
    "        q = q.view(B, S_q, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, S_k, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, S_k, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        if past_kv is not None:\n",
    "            k = torch.cat([past_kv[0], k], dim=2)\n",
    "            v = torch.cat([past_kv[1], v], dim=2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim) \n",
    "\n",
    "        attn_scores = F.softmax(scores, dim=-1)      \n",
    "        attn_scores = self.dropout(attn_scores)\n",
    "\n",
    "        context_vector = torch.matmul(attn_scores, v)          \n",
    "        context_vector = context_vector.transpose(1, 2).contiguous() \n",
    "        context_vector = context_vector.view(B, S_q, D)          \n",
    "\n",
    "        out = self.W_O(context_vector)                             \n",
    "        present_kv = (k, v)\n",
    "        return out, present_kv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0, \"emb_dim must be divisible by n_heads\"\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(emb_dim, emb_dim)  # (B, S_q, D) \n",
    "        self.W_K = nn.Linear(emb_dim, self.head_dim)  #  -> (B, S_k, h_dim) \n",
    "        self.W_V = nn.Linear(emb_dim, self.head_dim)  # -> (B, S_k, h_dim) \n",
    "        self.W_O = nn.Linear(emb_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, kv=None, mask=None):\n",
    "        B, S_q, D = x.shape\n",
    "        if kv is None:\n",
    "            kv = x\n",
    "        S_k = kv.shape[1]\n",
    "\n",
    "        q = self.W_Q(x)      # (B, S_q, D)\n",
    "        k = self.W_K(kv)     # (B, S_k, h_dim)\n",
    "        v = self.W_V(kv)     # (B, S_k, h_dim)\n",
    "\n",
    "        q = q.view(B, S_q, self.n_heads, self.head_dim)  # (B, S_q, H, h_dim)\n",
    "        k = k.view(B, S_k, 1, self.head_dim)  # (B, S_k, 1, h_dim)\n",
    "        v = v.view(B, S_k, 1, self.head_dim)  # (B, S_k, 1, h_dim)\n",
    "\n",
    "        q = q.transpose(1, 2)  # (B, H, S_q, h_dim)\n",
    "        k = k.transpose(1, 2)  # (B, 1, S_k, h_dim)\n",
    "        v = v.transpose(1, 2)  # (B, 1, S_k, h_dim)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, H, S_q, S_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            if mask.dtype != torch.bool:\n",
    "                mask = mask != 0\n",
    "\n",
    "            if mask.ndim == 2 and mask.shape == (S_q, S_k):   # causal/visibility mask\n",
    "                mask = mask.unsqueeze(0).unsqueeze(0)          # (1, 1, S_q, S_k)\n",
    "            elif mask.ndim == 2 and mask.shape == (B, S_k):    # key padding mask over K\n",
    "                mask = mask.unsqueeze(1).unsqueeze(2)          # (B, 1, 1, S_k)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mask shape: {mask.shape}\")\n",
    "            scores = scores.masked_fill(~mask, float(\"-inf\"))  # (B, H, S_q, S_k)\n",
    "\n",
    "        attn_scores = F.softmax(scores, dim=-1)      # for each query over all keys (B, H, S_q, S_k)\n",
    "        attn_scores = self.dropout(attn_scores)\n",
    "\n",
    "        # weighted sum of V\n",
    "        context_vector = torch.matmul(attn_scores, v)               # (B, H, S_q, h_dim)\n",
    "        context_vector = context_vector.transpose(1, 2).contiguous()  # (B, S_q, H, h_dim)\n",
    "        context_vector = context_vector.view(B, S_q, D)             # concat heads -> (B, S_q, D)\n",
    "\n",
    "        out = self.W_O(context_vector)                             # mix head information -> (B, S_q, D)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, kv_groups, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0, \"emb_dim must be divisible by n_heads\"\n",
    "        assert emb_dim % kv_groups == 0, \"emb_dim must be divisible by kv_groups\"\n",
    "        assert n_heads % kv_groups == 0, \"n_heads must be divisible by kv_groups\"\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.kv_groups = kv_groups\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(emb_dim, emb_dim)  # (B, S_q, D) \n",
    "        self.W_K = nn.Linear(emb_dim, self.kv_groups * self.head_dim) \n",
    "        self.W_V = nn.Linear(emb_dim, self.kv_groups * self.head_dim) \n",
    "        self.W_O = nn.Linear(emb_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, kv=None, mask=None):\n",
    "        B, S_q, D = x.shape\n",
    "        if kv is None:\n",
    "            kv = x\n",
    "        S_k = kv.shape[1]\n",
    "\n",
    "        q = self.W_Q(x)      # (B, S_q, D)\n",
    "        k = self.W_K(kv)     # (B, S_k, h_dim)\n",
    "        v = self.W_V(kv)     # (B, S_k, h_dim)\n",
    "\n",
    "        q = q.view(B, S_q, self.n_heads, self.head_dim)  # (B, S_q, H, h_dim)\n",
    "        k = k.view(B, S_k, self.kv_groups, self.head_dim)  # (B, S_k, G, h_dim)\n",
    "        v = v.view(B, S_k, self.kv_groups, self.head_dim)  # (B, S_k, G, h_dim)\n",
    "\n",
    "        q = q.transpose(1, 2)  # (B, H, S_q, h_dim)\n",
    "        k = k.transpose(1, 2)  # (B, G, S_k, h_dim)\n",
    "        v = v.transpose(1, 2)  # (B, G, S_k, h_dim)\n",
    "\n",
    "        if self.kv_groups != self.n_heads:\n",
    "            k = k.repeat(1, self.n_heads // self.kv_groups, 1, 1)\n",
    "            v = v.repeat(1, self.n_heads // self.kv_groups, 1, 1)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, H, S_q, S_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            if mask.dtype != torch.bool:\n",
    "                mask = mask != 0\n",
    "\n",
    "            if mask.ndim == 2 and mask.shape == (S_q, S_k):   # causal/visibility mask\n",
    "                mask = mask.unsqueeze(0).unsqueeze(0)          # (1, 1, S_q, S_k)\n",
    "            elif mask.ndim == 2 and mask.shape == (B, S_k):    # key padding mask over K\n",
    "                mask = mask.unsqueeze(1).unsqueeze(2)          # (B, 1, 1, S_k)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mask shape: {mask.shape}\")\n",
    "            scores = scores.masked_fill(~mask, float(\"-inf\"))  # (B, H, S_q, S_k)\n",
    "\n",
    "        attn_scores = F.softmax(scores, dim=-1)      # for each query over all keys (B, H, S_q, S_k)\n",
    "        attn_scores = self.dropout(attn_scores)\n",
    "\n",
    "        # weighted sum of V\n",
    "        context_vector = torch.matmul(attn_scores, v)               # (B, H, S_q, h_dim)\n",
    "        context_vector = context_vector.transpose(1, 2).contiguous()  # (B, S_q, H, h_dim)\n",
    "        context_vector = context_vector.view(B, S_q, D)             # concat heads -> (B, S_q, D)\n",
    "\n",
    "        out = self.W_O(context_vector)                             # mix head information -> (B, S_q, D)\n",
    "        return out\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutiheadLatentAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, down_dim, up_dim, rope_dim, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0, \"emb_dim must be divisible by n_heads\"\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.down_dim = down_dim\n",
    "        self.up_dim = up_dim\n",
    "        self.rope_dim = rope_dim\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "        self.v_head_dim = up_dim // n_heads\n",
    "\n",
    "        self.down_proj_kv = nn.Linear(emb_dim, down_dim)\n",
    "        self.down_proj_q = nn.Linear(emb_dim, down_dim)\n",
    "\n",
    "        self.up_proj_q = nn.Linear(down_dim, up_dim)\n",
    "        self.up_proj_k = nn.Linear(down_dim, up_dim)\n",
    "        self.up_proj_v = nn.Linear(down_dim, up_dim)\n",
    "\n",
    "        self.proj_qr = nn.Linear(down_dim, rope_dim * n_heads)\n",
    "        self.proj_kr_ = nn.Linear(emb_dim, rope_dim)\n",
    "\n",
    "        self.rope_q = RoPE(rope_dim * n_heads, n_heads)\n",
    "        self.rope_k = RoPE(rope_dim, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.res_dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(n_heads * self.v_head_dim, emb_dim)\n",
    "\n",
    "    def forward(self, h, kv=None, mask=None):\n",
    "        B, S_q, D = h.shape\n",
    "\n",
    "        if kv is None:\n",
    "            kv = h\n",
    "        S_k = kv.shape[1]\n",
    "\n",
    "        c_t_kv = self.down_proj_kv(kv) # (B, S_k, down_dim)\n",
    "        k_t_c = self.up_proj_k(c_t_kv) # (B, S_q, up_dim)\n",
    "        v_t_c = self.up_proj_v(c_t_kv) # (B, S_q, up_dim)\n",
    "\n",
    "        c_t_q = self.down_proj_q(h) # (B, S_q, down_dim)\n",
    "        q_t_c = self.up_proj_q(c_t_q) # (B, S_q, up_dim)\n",
    "\n",
    "        q_t_r = self.proj_qr(c_t_q) # (B, S_q, rope_dim * n_heads)\n",
    "        q_t_r = q_t_r.view(B, S_q, self.n_heads, self.rope_dim).transpose(1, 2) # (B, H, S_q, rope_dim)\n",
    "        q_t_r = self.rope_q(q_t_r) # (B, H, S_q, rope_dim)\n",
    "\n",
    "        k_t_r = self.proj_kr_(kv).unsqueeze(1) # (B, 1, S_k, rope_dim)\n",
    "        k_t_r = self.rope_k(k_t_r) # (B, 1, S_k, rope_dim)\n",
    "\n",
    "        q_t_c = q_t_c.view(B, S_q, self.n_heads, self.v_head_dim).transpose(1, 2) # (B, H, S_q, v_head_dim)\n",
    "        q = torch.cat([q_t_c, q_t_r], dim=-1) # (B, H, S_q, rope_dim + v_head_dim)\n",
    "\n",
    "        k_t_c = k_t_c.view(B, S_k, self.n_heads, self.v_head_dim).transpose(1, 2) # (B, H, S_k, v_head_dim)\n",
    "        k_t_r = k_t_r.expand(B, self.n_heads, S_k, self.rope_dim) # (B, H, S_k, rope_dim)\n",
    "        k = torch.cat([k_t_c, k_t_r], dim=-1) # (B, H, S_k, rope_dim + v_head_dim)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (math.sqrt(self.rope_dim) + math.sqrt(self.v_head_dim)) # (B, H, S_q, S_k)\n",
    "        scores = self.dropout(scores)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask[:, None, None, :]==0, float(\"-inf\"))\n",
    "        \n",
    "        attn_scores = F.softmax(scores, dim=-1) # (B, H, S_q, S_k)\n",
    "        attn_scores = self.dropout(attn_scores)\n",
    "\n",
    "        v_t_c = v_t_c.view(B, S_k, self.n_heads, self.v_head_dim).transpose(1, 2) # (B, H, S_k, v_head_dim)\n",
    "        context_vector = torch.matmul(attn_scores, v_t_c) # (B, H, S_q, v_head_dim)\n",
    "        context_vector = context_vector.transpose(1, 2).contiguous().view(B, S_q, D) # (B, S_q, up_dim)\n",
    "\n",
    "        out = self.fc(context_vector) # (B, S_q, D)\n",
    "        out = self.res_dropout(out)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
